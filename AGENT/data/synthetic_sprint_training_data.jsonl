{"sprint_name":"3.10-m3","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update Java version bundled found in the installer to a version >= 1.8u51\"","issue_description":"\"Update the bundled version of java to a version >= 1.8u51 (1.8 update 51), which fixes many security issues (http:\/\/www.oracle.com\/technetwork\/topics\/security\/cpujul2015-2367936.html).  Included in the security fixes is a fix for logjam (CVE-2015-4000)\"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"3.10-m3","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"SVN operations can hang in some cases when using svnkit with file:\/\/ protocol and long commit messages \"","issue_description":"\"See https:\/\/issues.tmatesoft.com\/issue\/SVNKIT-529    Example stack is:  {code}  2015-09-02 15:17:11,705 INFO  [InitPing2 aaa ] fisheye SvnTask-cancel - Cancelling Subversion operation : (InitPing2 aaa) svn log -r 7:HEAD file:\/\/repo@HEAD  2015-09-02 15:17:11,736 DEBUG [InitPing2 aaa ] fisheye SvnTask-cancel - Svn Thread stack:          at org.tmatesoft.svn.core.internal.io.fs.FSFile.read(FSFile.java:345)          at org.tmatesoft.svn.core.internal.io.fs.FSFile.readProperties(FSFile.java:260)          at org.tmatesoft.svn.core.internal.io.fs.revprop.SVNFSFSPackedRevProps.parseProperties(SVNFSFSPackedRevProps.java:341)          at org.tmatesoft.svn.core.internal.io.fs.revprop.SVNFSFSPackedRevProps.parseProperties(SVNFSFSPackedRevProps.java:140)          at org.tmatesoft.svn.core.internal.io.fs.FSFS.readPackedRevisionProperties(FSFS.java:679)          at org.tmatesoft.svn.core.internal.io.fs.FSFS.readRevisionProperties(FSFS.java:663)          at org.tmatesoft.svn.core.internal.io.fs.FSFS.getRevisionProperties(FSFS.java:636)          at org.tmatesoft.svn.core.internal.io.fs.FSLog.fillLogEntry(FSLog.java:347)          at org.tmatesoft.svn.core.internal.io.fs.FSLog.sendLog(FSLog.java:290)          at org.tmatesoft.svn.core.internal.io.fs.FSLog.doLogs(FSLog.java:256)          at org.tmatesoft.svn.core.internal.io.fs.FSLog.runLog(FSLog.java:172)          at org.tmatesoft.svn.core.internal.io.fs.FSRepository.logImpl(FSRepository.java:382)          at org.tmatesoft.svn.core.io.SVNRepository.log(SVNRepository.java:1038)          at org.tmatesoft.svn.core.internal.wc2.remote.SvnRemoteLog.run(SvnRemoteLog.java:181)          at org.tmatesoft.svn.core.internal.wc2.remote.SvnRemoteLog.run(SvnRemoteLog.java:35)          at org.tmatesoft.svn.core.internal.wc2.SvnOperationRunner.run(SvnOperationRunner.java:20)          at org.tmatesoft.svn.core.wc2.SvnOperationFactory.run(SvnOperationFactory.java:1258)          at org.tmatesoft.svn.core.wc2.SvnOperation.run(SvnOperation.java:294)          at org.tmatesoft.svn.core.javahl17.SVNClientImpl.logMessages(SVNClientImpl.java:293)          at org.apache.subversion.javahl.SVNClient.logMessages(SVNClient.java:82)          at com.cenqua.fisheye.svn.SvnThrottledClient$2.call(SvnThrottledClient.java:143)          at com.cenqua.fisheye.svn.SvnThrottledClient$2.call(SvnThrottledClient.java:138)          at java.util.concurrent.FutureTask.run(FutureTask.java:262)          at com.cenqua.fisheye.svn.SvnTask.access$101(SvnTask.java:14)          at com.cenqua.fisheye.svn.SvnTask$1.run(SvnTask.java:36)          at com.cenqua.fisheye.util.NamedExecution.run(NamedExecution.java:27)          at com.cenqua.fisheye.svn.SvnTask.run(SvnTask.java:31)          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)          at java.lang.Thread.run(Thread.java:744)  {code}\"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"3.10-m3","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Invisible error messages in admin pages\"","issue_description":"\"See !invisibleErrorMessage.png|thumbnail!\"","issue_type":"Bug","issue_priority":"Low","story_points":0.5}
{"sprint_name":"3.10-m3","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"NPS warnings on instance startup\"","issue_description":"\"When starting a clean 3.9.1 instance I see several warnings like:  {code}  2015-09-10 16:50:40,581 WARN  [qtp138776324-132 ] com.atlassian.nps.plugin.storage.UserServerStorageService UserServerStorageService-getSetting - Couldn't check the NPS settings. This can safely be ignored during plugin shutdown. Detail: null  {code}    Once I setup instance and log in with admin password, I see different warnings:  {code}  2015-09-10 16:55:03,212 WARN  [qtp138776324-176 ] com.atlassian.nps.plugin.storage.UserServerStorageService UserServerStorageService-getSetting - Couldn't check the NPS settings. This can safely be ignored during plugin shutdown. Detail: String value of UserKey '$admin$' is not an integer  {code}    Few of them are logged for every web request, e.g. dashboard page.    Once I am logged in as named user warnings no longer appear.\"","issue_type":"Bug","issue_priority":"Medium","story_points":0.5}
{"sprint_name":"3.10-m3","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add a link to include\/exclude and patterns CAC page \"","issue_description":"\"h3. Problem Definition  The \"\"Include\/Exclude Paths\"\" section (both repository defaults and individual repository) can be very confusing when it comes to getting the correct patterns defined for indexing and can lead to \"\"missing revisions\"\" or \"\"excluded files\"\" errors in the logs.   h3. Suggested Solution  Have a link to the CAC guides from the application \"\"Include\/Exclude Paths\"\" screens :  https:\/\/confluence.atlassian.com\/fisheye\/include-exclude-paths-298976903.html  https:\/\/confluence.atlassian.com\/fisheye\/pattern-matching-guide-298976797.html  to help give the users a reference point.   \"","issue_type":"Suggestion","issue_priority":"","story_points":0.5}
{"sprint_name":"3.10-m3","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"OutOfMemoryError when Start Revision, Initial Import: \"\"No initial import\"\" and repository path are set\"","issue_description":"\"Apparently when Start revision is set and initial import is set to \"\"No initial import\"\", FishEye may crash with OutOfMemoryError with large repositories (with a huge number of files at the start-rev revision).    FE-4773 fixed the issue for repositories where repository path (Repository Config \/ SCM Details \/ SVN Connection Details \/ Path) is empty, but when it is not empty FishEye would still attempt to do initial import.\"","issue_type":"Bug","issue_priority":"Low","story_points":2.0}
{"sprint_name":"3.10-m3","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Dead Link for Allow 2-Legged OAuth in FishEye Crucible\"","issue_description":"\"h3. Summary    The _Allow 2-Legged OAuth_ link in the app links pop up dialogue leads to a dead link;    h3. Steps to Reproduce  # Set ip up application link  # Edit the application link  # Follow the link on Outgoing Authentication > OAuth tab    !Screen Shot 2015-08-17 at 11.33.20.png|thumbnail!    h3. Expected Result  The user is directed to https:\/\/confluence.atlassian.com\/display\/APPLINKS\/Configuring+OAuth+Authentication+for+an+Application+Link instead.    h3. Actual Result    The user is directed to https:\/\/confluence.atlassian.com\/display\/CRUCIBLE038\/Configuring+OAuth+Authentication+for+an+Application+Link      h3. Notes  On the older versions this looks like it should be https:\/\/confluence.atlassian.com\/display\/FISHEYE034\/Configuring+OAuth+Authentication+for+an+Application+Link - with the version number replaced with the appropriate one    There is also no mention of the use of 2-legged OAuth or 2-legged OAuth with Impersonation on this page - [Configuring OAuth Authentication for an Application Link|https:\/\/confluence.atlassian.com\/display\/APPLINKS\/Configuring+OAuth+Authentication+for+an+Application+Link], this does appear to be present on the following documentation though - [Configuring Authentication for an Application Link|https:\/\/confluence.atlassian.com\/display\/APPLINKS\/Configuring+Authentication+for+an+Application+Link].   \"","issue_type":"Bug","issue_priority":"Low","story_points":0.5}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Create presentation for SST on deblending in the stack\"","issue_description":"\"Create a presentation for the LSST System Science Team (SST) on deblending in the stack. It has been requested that this presentation includes a general introduction to blending in the science pipelines as well as an update on the latest developments.\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add two-component models to MultiProFitTask\"","issue_description":"\"Add flag(s) for fitting two-component models in MultiProFitConfig so they can be configured in MultiProFitTask for use in e.g. DM-23169.\"","issue_type":"Story","issue_priority":"","story_points":3.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"validate_drp crashes when trying to apply external skyWcs\"","issue_description":"\"RC2 processing on DM-23121 crashed when trying to apply jointcal wcs in validate_drp.  https:\/\/lsstc.slack.com\/archives\/C4JQP6FRS\/p1580160187055200    Fix was illustrated by [~price]:    {code:java}  diff --git a\/python\/lsst\/validate\/drp\/matchreduce.py b\/python\/lsst\/validate\/drp\/matchreduce.py  index b2e9e34..5ab9804 100644  --- a\/python\/lsst\/validate\/drp\/matchreduce.py  +++ b\/python\/lsst\/validate\/drp\/matchreduce.py  @@ -313,7 +313,7 @@ def _loadAndMatchCatalogs(repo, dataIds, matchRadius,               \/ tmpCat['base_PsfFlux_instFluxErr']           if doApplyExternalSkyWcs:  -            afwTable.wcsUtils.updateSourceCoords(wcs, tmpCat)  +            afwTable.updateSourceCoords(wcs, tmpCat)           photoCalib.instFluxToMagnitude(tmpCat, \"\"base_PsfFlux\"\", \"\"base_PsfFlux\"\")           if not skipNonSrd:               tmpCat['slot_ModelFlux_snr'][:] = (tmpCat['slot_ModelFlux_instFlux'] \/  {code}  \"","issue_type":"Bug","issue_priority":"","story_points":0.5}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Store MultiComponentSource components in catalog\"","issue_description":"\"The current output from both {{meas_deblender}} and {{meas_extensions_scarlet}} is a catalog with a record for each \"\"parent\"\" blend and a record for each \"\"peak\"\" in the parent. Since scarlet allows sources to have multiple components, we should propagate this information in the catalog, adding a row for each component, setting the \"\"parent\"\" to the source containing the components and add a field to the catalog to list whether a source is a blend, source, or component, to make downstream processing easier.\"","issue_type":"Story","issue_priority":"","story_points":5.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add matchedVisitMetric configs to obs_subaru\"","issue_description":"\"ValidateDrp slipped into the RC2, and Hsin-Fang had just applied desired configs on the command line. The config definitions changed over the last year. We should track these \"\"official configs\"\" in obs_subaru like we do for every other task. \"","issue_type":"Story","issue_priority":"","story_points":1.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Use correct weight maps in scarlet\"","issue_description":"\"The ADAM algorithm in scarlet requires accurate weights maps (inverse variance) for optimal convergence. Currently {{meas_extensions_scarlet}} used the inverse variance from each {{MultibandExposure}} as the weight for a blend, then masked the image with the footprint and any masked pixels. But the stack already interpolates pixels for cosmic rays, saturated pixels, etc and updates the variance accordingly, so masking out those pixels in the relevant bands was making the task more difficult on the deblender than necessary. This ticket is to implement the proper weight maps for scarlet.\"","issue_type":"Story","issue_priority":"","story_points":6.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Fit DC2 with MultiProFit\"","issue_description":"\"Fit some DC2 patches with MultiProFit and investigate the best way to analyze the results (match & compare to inputs).\"","issue_type":"Story","issue_priority":"","story_points":20.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Fix \"\"unordered\"\" map documentation in DetectorCollection getters\"","issue_description":"\"from [~<USER> on Slack  {quote}  The comment just above this method says \"\"Get an unordered map\"\", but it returns a binary-tree map instead:  [https:\/\/github.com\/lsst\/afw\/blob\/6637c4fb6fa5813268496d76dcd9e97cbd417bcc\/include\/lsst\/afw\/cameraGeom\/DetectorCollection.h#L62]  {quote}    We should just fix the docs; we probably have code that now depends on these being ordered, and it's not unreasonable for them to be ordered.\"","issue_type":"Story","issue_priority":"","story_points":1.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Augment ObjectTable to be useable for QA\"","issue_description":"\"I went through the exercise of trying to use the ObjectTable for some QA last week. I had to add a lot of columns we use regularly like per-band CalibFlux, and per-band PixelFlags.     In addition to these, I'm going to add the columns that pipe_analysis uses as well.  ObjectTable is going to bitrot if its too skinny to be used in practice.     ideally, it'd be something that [~<USER> could base the example qa-drilldown site on.       \"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update python types for matchVisits and objectTable\"","issue_description":"\"I missed a couple changes on DM-16234. Wasn't aware that matchVisits was being used at the time.\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"HSC-Y failed on w_2020_02\"","issue_description":"\"Copied and pasted:    ----------    Problem with HSC-Y (this band only) during executing singleFrameDriver.    cmd line:\u00a0    \u00a0  {noformat}singleFrameDriver.py \/datasets\/hsc\/repo --calib \/datasets\/hsc\/repo\/CALIB --rerun RC\/w_2020_02\/DM-23066-sfm --batch-type slurm --mpiexec='-bind-to socket' --job sfm9615HSC-Y --cores 70 --time 900 --id visit=380^384^388^404^408^424^426^436^440^442^446^452^456^458^462^464^468^470^472^474^478^27032^27034^27042^27066^27068 ccd=0..8^10..103  {noformat}  In\u00a0job_sfm9615HSC-Y.log\u00a0(example, repeated for every visit):    \u00a0  {noformat}18372 INFO\u00a0 2020-01-15T09:11:42.405-0600 singleFrameDriver ({'visit': 27068, 'ccd': 101, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0})(parallel.py:519)- lsst-verify-worker07:18372: Finished processing {'visit': 27068, 'ccd': 101, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}  18372 FATAL 2020-01-15T09:11:42.405-0600 singleFrameDriver ({'visit': 27068, 'ccd': 101, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0})(cmdLineTask.py:397)- Failed on dataId={'visit': 27068, 'ccd': 101, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}: TypeError: '>=' not supported between instances of 'method' and 'datetime.datetime'  18366 WARN\u00a0 2020-01-15T09:11:42.595-0600 CameraMapper ({'visit': 27068, 'ccd': 100, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0})(cameraMapper.py:1147)- Cannot create SkyWcs using VisitInfo and Detector, using metadata-based SkyWcs: Cannot create SkyWcs from camera geometry: rotator angle defined using RotType=RotType.UNKNOWN instead of SKY.  18366 INFO\u00a0 2020-01-15T09:11:42.605-0600 singleFrameDriver ({'visit': 27068, 'ccd': 100, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0})(parallel.py:519)- lsst-verify-worker07:18366: Finished processing {'visit': 27068, 'ccd': 100, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}  {noformat}  \u00a0    In\u00a0sfm9615HSC-Y.o228004 file (example):    \u00a0  {noformat}18381 WARN\u00a0 2020-01-15T09:11:42.727-0600 CameraMapper: Cannot create SkyWcs using VisitInfo and Detector, using metadata-based SkyWcs: Cannot create SkyWcs from camera geometry: rotator angle defined using RotType=RotType.UNKNOWN instead of SKY.  18381 INFO\u00a0 2020-01-15T09:11:42.736-0600 singleFrameDriver: lsst-verify-worker07:18381: Finished processing {'visit': 27068, 'ccd': 103, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}  18381 FATAL 2020-01-15T09:11:42.737-0600 singleFrameDriver: Failed on dataId={'visit': 27068, 'ccd': 103, 'field': 'SSP_WIDE', 'dateObs': '2015-03-29', 'pointing': 1183, 'filter': 'HSC-Y', 'taiObs': '2015-03-29', 'expTime': 200.0}: TypeError: '>=' not supported between instances of 'method' and 'datetime.datetime'  Traceback (most recent call last):  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/pipe_base\/19.0.0-5-gfe96e6c+2\/python\/lsst\/pipe\/base\/cmdLineTask.py\"\", line 388, in __call__  \u00a0 \u00a0 result = self.runTask(task, dataRef, kwargs)  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/pipe_base\/19.0.0-5-gfe96e6c+2\/python\/lsst\/pipe\/base\/cmdLineTask.py\"\", line 447, in runTask  \u00a0 \u00a0 return task.runDataRef(dataRef, **kwargs)  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/pipe_drivers\/19.0.0-2-gd955cfd+17\/python\/lsst\/pipe\/drivers\/singleFrameDriver.py\"\", line 70, in runDataRef  \u00a0 \u00a0 return self.processCcd.runDataRef(sensorRef)  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/pipe_base\/19.0.0-5-gfe96e6c+2\/python\/lsst\/pipe\/base\/timer.py\"\", line 150, in wrapper  \u00a0 \u00a0 res = func(self, *args, **keyArgs)  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/pipe_tasks\/19.0.0-10-g448f008b+3\/python\/lsst\/pipe\/tasks\/processCcd.py\"\", line 181, in runDataRef  \u00a0 \u00a0 exposure = self.isr.runDataRef(sensorRef).exposure  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/pipe_base\/19.0.0-5-gfe96e6c+2\/python\/lsst\/pipe\/base\/timer.py\"\", line 150, in wrapper  \u00a0 \u00a0 res = func(self, *args, **keyArgs)  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/ip_isr\/19.0.0-7-gea0a0fe\/python\/lsst\/ip\/isr\/isrTask.py\"\", line 1502, in runDataRef  \u00a0 \u00a0 isrData = self.readIsrData(sensorRef, ccdExposure)  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/ip_isr\/19.0.0-7-gea0a0fe\/python\/lsst\/ip\/isr\/isrTask.py\"\", line 985, in readIsrData  \u00a0 \u00a0 strayLightData = self.strayLight.readIsrData(dataRef, rawExposure)  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/obs_subaru\/19.0.0-7-g2f4ee70a\/python\/lsst\/obs\/subaru\/strayLight\/yStrayLight.py\"\", line 59, in readIsrData  \u00a0 \u00a0 if not self.check(rawExposure):  \u00a0 File \"\"\/software\/lsstsw\/stack_20191101\/stack\/miniconda3-4.5.12-4d7b902\/Linux64\/obs_subaru\/19.0.0-7-g2f4ee70a\/python\/lsst\/obs\/subaru\/strayLight\/yStrayLight.py\"\", line 73, in check  \u00a0 \u00a0 if exposure.getInfo().getVisitInfo().getDate().toPython >= datetime.datetime(2018, 1, 1):  TypeError: '>=' not supported between instances of 'method' and 'datetime.datetime'  root ERROR: 2678 dataRefs failed; not exiting as --noExit was set  <string>:118: FutureWarning: Config field processCcd.isr.doAddDistortionModel is deprecated: Camera geometry is incorporated when reading the raw files. This option no longer is used, and will be removed after v19.  {noformat}  \u00a0\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add sky objects to the single frame processing step\"","issue_description":"\"Currently, sky objects are implemented on the coadds (see ticket DM-4840). For various QA reasons it would be beneificial to have sky objects for single frame imaging also. This ticket adds sky objects at the single frame visit level. Such sources are termed 'sky sources', to distinguish them from the currently existing sky objects output by the stack and in keeping with processing nomenclature standards.     A community post highlighting this change is located here: https:\/\/community.lsst.org\/t\/sky-sources-added-to-single-frame-processing\/4137\"","issue_type":"Story","issue_priority":"","story_points":12.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Delete commented code in tests\/test_matchBackgrounds.py\"","issue_description":"\"Exercise ticket to learn workflow\"","issue_type":"Story","issue_priority":"","story_points":1.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Test fgcmcal on NB0387 HSC data\"","issue_description":"\"[~<USER> et al. have requested that {{fgcmcal}} be run on NB0387 which has proved to be difficult to calibrate in the past.  As of DM-22955, the spatially varying narrow-band filters are now in {{obs_subaru}}, making it possible for {{fgcmcal}} to be run on these data.  In addition, [~<USER> has run {{singleFrameDriver}} on some NB0387 data from PDR2 over COSMOS, which is on {{lsst-dev}} at {{\/datasets\/hsc\/repo\/rerun\/private\/<USER>PDR2\/w_2020_01\/}} (see https:\/\/lsstc.slack.com\/archives\/C2JPXB4HG\/p1578945289050000).      This ticket will report on the progress from a test run on COSMOS with NB0387 (from PDR2) + g + r + i + z + y (from RC2), and will also encompass any updates to {{fgcm}} and {{fgcmcal}} that are necessary to make this work.\"","issue_type":"Story","issue_priority":"","story_points":6.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Fix association of flag columns to forced_src catalogs\"","issue_description":"\"The {{coaddAnalysis.py}} script distinguishes between \"\"unforced\"\" (i.e. the {{\\*_meas}} dataset type) and \"\"forced\"\" (i.e the {{\\*_forced_src}} dataset type). The latter table does not include some of the flag columns that are useful for sample sub-selection. The desired columns are thus copied to the \"\"forced\"\" catalog within the script.\u00a0 \u00a0Most of these columns should be coming from the {{\\*_ref}} dataset, as it is those that are appropriate for the forced catalogs. \u00a0Some of those that should be coming from {{\\*_ref}} are coming from {{\\*_meas}} and thus\u00a0can have the wrong value (it's quite rare but, for example, for two random patches, the {{detect_isPatchInner}} is different for 1 and 6 sources). \u00a0However,\u00a0we want the various \"\"calib\\*\"\" columns to come from the {{\\*_meas}} catalog as these\u00a0reflect what actually occurred during SFM per band (which is always what we want\u00a0for the calib* columns rather than assigning the specific value associated with\u00a0the forced filter to all other filters). \u00a0Here we should make the column copying more\u00a0explicit and in accordance with the above desiderata.    \u00a0\"","issue_type":"Story","issue_priority":"","story_points":10.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Understand problem with modelling bright stars' wings\"","issue_description":"\"Understand and explore the issue with bright star wings.    \u00a0    Quick summary:   * Started exploratory work (and familiarization with stack functions\/data products) by looking at bright stars on both single exposures and coadds, both HSC Wide and DUD fields, etc.   * Looked into current bright star masks for HSC, both the older Arcturus (Coupon et al., 2018) and the more recent\/larger ones by Andy Goulding. Had discussions with the latter about their creation. Played around with the masks. In particular, how to use them as a stand-in for using external catalogs to identify and extract bright stars.     * Searched for existing approaches; found they typically rely on stacking large number of bright stars (latest reference being Infante-Sainz et al., 2020, with application to SDSS).   * During analysis sprint, studied radial detection density around bright stars; can be used to determine if future bright star wing models allow for smaller masks.    \u00a0    The conclusion is a plan for a first-step model, namely that of Infante-Sainz 2020, but using stack machinery, which will already increase flexibility (eg by allowing for choice of stacking statistic). See DM-23785 and DM-23786.    \u00a0    With these deployed, we can start studying main sources of variability in the extended PSF: seeing, bright object color, position within focal plane? As well as relationship between number of bright objects used to build model, and its quality.    Combining these should lead to natural next steps for improvement.\"","issue_type":"Story","issue_priority":"","story_points":24.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add ability for fgcmcal to do calibrations on local background-corrected fluxes\"","issue_description":"\"Investigation of calibration errors has shown that improvements can be made by performing local background subtraction on the aperture fluxes.  These will bring the faint stars more in line with the bright stars, and should only improve calibrations (decreasing bias and scatter for bright stars), and this ticket does not imply that these corrections should necessarily be made outside of calibration, or to fluxes other than aperture fluxes used for calibration.\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Investigate galaxy bias introduced with scarlet 1.0\"","issue_description":"\"In DM-22137 it was discovered that the newer version of scarlet introduces a large (~500 mmag) bias and increased scatter in galaxy photometry that is not present for stellar sources. This ticket is to identify why scarlet is failing for most galaxies while performing so well for stellar sources.\"","issue_type":"Story","issue_priority":"","story_points":6.0}
{"sprint_name":"DRP S20-2 (Jan)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Remove SubaruMakeCoaddTempExpTask after S19A AND problem  fixed upstream\"","issue_description":"\"Resolution to DM-19841 was to add hack in makeCoaddTempExp to update masks on ccd 33 and 9 on the fly. Once S19A is over and the problem is fixed upstream during defect interpolation this Task override should be removed.\"","issue_type":"Story","issue_priority":"","story_points":8.0}
{"sprint_name":"ESB Dolomite 2","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Create Scatter-Gather component for parallel multicasting\"","issue_description":"\"The <all> router currently in Mule makes a good job at doing message multicasting. However, since its processing is sequential, it's not a good fit for some use cases.    Implement a scatter-gather component as described in https:\/\/github.com\/mulesoft\/mule\/wiki\/%5BMULE-3.5-February-2014%5D-Scatter-Gather for parallel multicasting\"","issue_type":"Enhancement Request","issue_priority":"Major","story_points":8.0}
{"sprint_name":"ESB Dolomite 2","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Analyse the changes to have MULE_HOME and MULE_BASE\"","issue_description":"\"We need to review the cost and the impact of making the change for MULE-7182 to have Studio starting their apps in the same way Mule Starts them\"","issue_type":"Task","issue_priority":"Major","story_points":8.0}
{"sprint_name":"ESB Dolomite 2","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Remove installer specific behaviour from SwitchVersion script\"","issue_description":"\"Remove installer code from this file: https:\/\/github.com\/mulesoft\/mule\/blob\/mule-3.x\/buildtools\/scripts\/SwitchVersion.groovy\"","issue_type":"Task","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"ESB Dolomite 2","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Create mule domain + domain applications project creation using maven artifact.\"","issue_description":"\"This should be kind of an extension of MULE-7138 but should also allow to have a multi module project where one of the project is defined as the domain and the other projects are mule applications that belong to that domain.    This artifact should create a package containing the domain + the apps that during deployment it will first create the domain and then deploy the applications.\"","issue_type":"Task","issue_priority":"Critical","story_points":8.0}
{"sprint_name":"ESB Dolomite 2","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"FTP Inbound endpoint fails when reading empty file\"","issue_description":"\"Put an empty file into FTP inbound path\"","issue_type":"Bug","issue_priority":"Critical","story_points":5.0}
{"sprint_name":"International 3.9 - Sprint 5","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"TinyMCE editor font sizes are too small in Classic theme (and other child themes)\"","issue_description":"\"Currently, for child themes to have editor SCSS styles included (eg Boost includes Bootstrap), they must have them configured, and include their own SCSS file. The result is that themes such as Classic use TinyMCE's default fonts, which do not reflect what is displayed on the page once the input is saved, and in some cases are small enough to be difficult to read. This appears to be something I did not factor in as part of\u00a0 MDL-62968.    I think rather than requiring copy\/pasting of content into every theme, child themes should only need to include editor SCSS if they wish to override the parent theme, otherwise they should default to the value being used by the parent (this should also stop this problem arising again, as it has in the past with clean\/bootstrapbase MDL-39424 and Boost MDL-60588).    Example (using TinyMCE's default in Classic):    !MDL-67364-TinyMCE_headings_before.png|thumbnail!\"","issue_type":"Bug","issue_priority":"Minor","story_points":0.0}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Remove dependency on javax.xml.bind.DatatypeConverter\"","issue_description":"\"In Java 9, the javax.xml.bind.DatatypeConverter class is part of the the java.xml.bind module, which is not a dependency of the java.se module (though it is a dependency of java.se.ee).  To maximize deployment opportunities for the driver, the dependency on this module should be eliminated.\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add Document.get(key, defaultValue)\"","issue_description":"\"Based on the PR: https:\/\/github.com\/mongodb\/mongo-java-driver\/pull\/391\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"fsyncunlock does not work on secondaries with 3.2.x\"","issue_description":"\"I am using java driver version 3.4.2    Fsyncunlock does not work on the secondary with 3.2.3 & 3.2.12. It works fine on the primary.    Here is the stack trace of the error.    {code}  21 Apr 2017 22:11:42,014 ERROR ~ Application-handleException: Timed out after 10000 ms while waiting for a server that matches WritableServerSelector. Client view of cluster state i  s {type=REPLICA_SET, servers=[{address=127.0.0.1:27017, type=REPLICA_SET_SECONDARY, roundTripTime=0.2 ms, state=CONNECTED}]  21 Apr 2017 22:11:42,014 ERROR ~ Timed out after 10000 ms while waiting for a server that matches WritableServerSelector. Client view of cluster state is {type=REPLICA_SET, servers=  [{address=127.0.0.1:27017, type=REPLICA_SET_SECONDARY, roundTripTime=0.2 ms, state=CONNECTED}]  21 Apr 2017 22:11:42,015 ERROR ~ com.mongodb.MongoTimeoutException: Timed out after 10000 ms while waiting for a server that matches WritableServerSelector. Client view of cluster s  tate is {type=REPLICA_SET, servers=[{address=127.0.0.1:27017, type=REPLICA_SET_SECONDARY, roundTripTime=0.2 ms, state=CONNECTED}]          at com.mongodb.connection.BaseCluster.createTimeoutException(BaseCluster.java:377)          at com.mongodb.connection.BaseCluster.selectServer(BaseCluster.java:104)          at com.mongodb.binding.ClusterBinding$ClusterBindingConnectionSource.<init>(ClusterBinding.java:75)          at com.mongodb.binding.ClusterBinding$ClusterBindingConnectionSource.<init>(ClusterBinding.java:71)          at com.mongodb.binding.ClusterBinding.getWriteConnectionSource(ClusterBinding.java:68)          at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:411)          at com.mongodb.operation.FsyncUnlockOperation.execute(FsyncUnlockOperation.java:41)          at com.mongodb.operation.FsyncUnlockOperation.execute(FsyncUnlockOperation.java:38)          at com.mongodb.Mongo.execute(Mongo.java:845)          at com.mongodb.Mongo.unlock(Mongo.java:644)          at controllers.MongoDBInstance.fsyncAndLock(MongoDBInstance.java:861)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)          at java.lang.reflect.Method.invoke(Method.java:497)          at play.mvc.ActionInvoker.invokeWithContinuation(ActionInvoker.java:548)          at play.mvc.ActionInvoker.invoke(ActionInvoker.java:502)          at play.mvc.ActionInvoker.invokeControllerMethod(ActionInvoker.java:478)          at play.mvc.ActionInvoker.invokeControllerMethod(ActionInvoker.java:473)          at play.mvc.ActionInvoker.invoke(ActionInvoker.java:161)          at play.server.PlayHandler$NettyInvocation.execute(PlayHandler.java:257)          at play.Invoker$Invocation.run(Invoker.java:278)          at play.server.PlayHandler$NettyInvocation.run(PlayHandler.java:235)          at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)          at java.util.concurrent.FutureTask.run(FutureTask.java:266)          at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)          at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)          at java.lang.Thread.run(Thread.java:745)    {code}\"","issue_type":"Bug","issue_priority":"Minor - P4","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"org.bson.json.JsonParseException: JSON reader was expecting a value but found '}'.\"","issue_description":"\"The following code works prior to version 3.4. but does not in 3.5.0-Snapshot    {code:java}      @Test      public void testDecode() {          StringWriter stringWriter = new StringWriter();          JsonWriter writer = new JsonWriter(stringWriter, new JsonWriterSettings(true));          CodecRegistry codecRegistry = CodecRegistries.fromRegistries(MongoClient.getDefaultCodecRegistry());          Codec<Document> documentCodec = codecRegistry.get(Document.class);            Document document = new Document(\"\"dateList\"\", Arrays.asList(new Date(), new Date()));          documentCodec.encode(writer, document, EncoderContext.builder().build());            Document decoded = documentCodec.decode(new JsonReader(stringWriter.toString()), DecoderContext.builder().build());      }  {code}    While decoding, it throws an exception after the first date is read.    org.bson.json.JsonParseException: JSON reader was expecting a value but found '}'.\"","issue_type":"Bug","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Connection failure from Android\"","issue_description":"\"Java driver 3.3.0 works fine and I tried updating to 3.4.2 but the app crashes with the following exception:    java.lang.ExceptionInInitializerError       at com.mongodb.connection.InternalStreamConnectionFactory.<init>(InternalStreamConnectionFactory.java:41)       at com.mongodb.connection.DefaultClusterableServerFactory.create(DefaultClusterableServerFactory.java:68)       at com.mongodb.connection.BaseCluster.createServer(BaseCluster.java:360)       at com.mongodb.connection.SingleServerCluster.<init>(SingleServerCluster.java:54)       at com.mongodb.connection.DefaultClusterFactory.create(DefaultClusterFactory.java:114)       at com.mongodb.Mongo.createCluster(Mongo.java:744)       at com.mongodb.Mongo.createCluster(Mongo.java:728)       at com.mongodb.Mongo.<init>(Mongo.java:293)       at com.mongodb.Mongo.<init>(Mongo.java:288)       at com.mongodb.Mongo.<init>(Mongo.java:284)       at com.mongodb.MongoClient.<init>(MongoClient.java:179)       at com.mongodb.MongoClient.<init>(MongoClient.java:136)       \u2026 app code ...       at java.lang.Thread.run(Thread.java:818)    Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'java.security.CodeSource java.security.ProtectionDomain.getCodeSource()' on a null object reference       at com.mongodb.connection.ClientMetadataHelper.getDriverVersion(ClientMetadataHelper.java:111)       at com.mongodb.connection.ClientMetadataHelper.getDriverInformation(ClientMetadataHelper.java:201)       at com.mongodb.connection.ClientMetadataHelper.addDriverInformation(ClientMetadataHelper.java:182)       at com.mongodb.connection.ClientMetadataHelper.<clinit>(ClientMetadataHelper.java:64)       at com.mongodb.connection.InternalStreamConnectionFactory.<init>(InternalStreamConnectionFactory.java:41)        at com.mongodb.connection.DefaultClusterableServerFactory.create(DefaultClusterableServerFactory.java:68)        at com.mongodb.connection.BaseCluster.createServer(BaseCluster.java:360)        at com.mongodb.connection.SingleServerCluster.<init>(SingleServerCluster.java:54)        at com.mongodb.connection.DefaultClusterFactory.create(DefaultClusterFactory.java:114)        at com.mongodb.Mongo.createCluster(Mongo.java:744)        at com.mongodb.Mongo.createCluster(Mongo.java:728)        at com.mongodb.Mongo.<init>(Mongo.java:293)        at com.mongodb.Mongo.<init>(Mongo.java:288)        at com.mongodb.Mongo.<init>(Mongo.java:284)        at com.mongodb.MongoClient.<init>(MongoClient.java:179)        at com.mongodb.MongoClient.<init>(MongoClient.java:136)        \u2026 app code ...       at java.lang.Thread.run(Thread.java:818) \"","issue_type":"Bug","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Exception com.mongodb.MongoInternalException has error in its message\"","issue_description":"\"The exception message suggests to set sslInvalidHostNameAllowed property to false when using java 1.6 while the code is checking for the opposite condition in https:\/\/github.com\/mongodb\/mongo-java-driver\/blob\/master\/driver-core\/src\/main\/com\/mongodb\/connection\/SslSettings.java#L124  :  {code:java}   SslSettings(final Builder builder) {          enabled = builder.enabled;          invalidHostNameAllowed = builder.invalidHostNameAllowed;          if (enabled && !invalidHostNameAllowed) {              if (System.getProperty(\"\"java.version\"\").startsWith(\"\"1.6.\"\")) {                  throw new MongoInternalException(\"\"By default, SSL connections are only supported on Java 7 or later.  If the application \"\"                                                   + \"\"must run on Java 6, you must set the SslSettings.invalidHostNameAllowed property to \"\"                                                   + \"\"false\"\");              }          }      }  {code}  \"","issue_type":"Bug","issue_priority":"Trivial - P5","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Investigate removing classmate dependency\"","issue_description":"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"NPE in PojoCodecProvider\"","issue_description":"\"The check for packages.contains(clazz.getPackage().getName()))) can cause an NPE when the clazz is a primitive, e.g. byte[], because those classes have null packages.\"","issue_type":"Bug","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Handle out-of-order keys in Extended JSON\"","issue_description":"\"The Extended JSON specification requires that the $-prefixed keys can appear in any order, e.g.    {code:json}  {b : {$binary : \"\"AQID=\"\", type : \"\"02\"\"}}  {b : {$type : \"\"02\"\", $binary : \"\"AQID=\"\"}}  {code}    Currently JsonReader requires the keys to be in the specified order, so the second case above will be parsed as a normal document rather than as a BSON Binary. \"","issue_type":"Improvement","issue_priority":"Minor - P4","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Logging in org.bson should also use slf4j if possible\"","issue_description":"\"Would make configuring logging simpler.\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"aggregation on view doesn't work \"","issue_description":"\"Aggregation on [view|(https:\/\/docs.mongodb.com\/manual\/reference\/method\/db.createView\/#db.createView)] doesn't work when run with java driver, but works fine in shell.     sample code to reproduce the issue:    {code:java}    MongoClient client = new MongoClient(\"\"localhost\"\", 27017);  SimpleMongoDbFactory factory = new SimpleMongoDbFactory(client, \"\"databaseName\"\");  MongoTemplate mongoTemplate = new MongoTemplate(factory);  List<DBObject> pipeline = new ArrayList<>();  BasicDBObject sort = new BasicDBObject(\"\"$sort\"\", new BasicDBObject(\"\"_id\"\", 1));  pipeline.add(sort);    AggregationOutput output = mongoTemplate.getCollection(\"\"viewCollection\"\").aggregate(pipeline);  for (DBObject obj : output.results()) {     String id = (String) obj.get(\"\"_id\"\");     System.out.println(\"\"ID IS \"\" + id);  }  {code}             Error shows up with both *Cursor* and *AggregationOutput*    *stack trace:*    Exception in thread \"\"main\"\" org.bson.BsonInvalidOperationException: Document does not contain key result   at org.bson.BsonDocument.throwIfKeyAbsent(BsonDocument.java:844)   at org.bson.BsonDocument.getArray(BsonDocument.java:147)   at com.mongodb.operation.BsonDocumentWrapperHelper.toList(BsonDocumentWrapperHelper.java:28)   at com.mongodb.operation.AggregateOperation.createQueryResult(AggregateOperation.java:356)   at com.mongodb.operation.AggregateOperation.access$700(AggregateOperation.java:67)   at com.mongodb.operation.AggregateOperation$3.apply(AggregateOperation.java:367)   at com.mongodb.operation.AggregateOperation$3.apply(AggregateOperation.java:364)   at com.mongodb.operation.CommandOperationHelper.executeWrappedCommandProtocol(CommandOperationHelper.java:216)   at com.mongodb.operation.CommandOperationHelper.executeWrappedCommandProtocol(CommandOperationHelper.java:207)   at com.mongodb.operation.CommandOperationHelper.executeWrappedCommandProtocol(CommandOperationHelper.java:113)   at com.mongodb.operation.AggregateOperation$1.call(AggregateOperation.java:257)   at com.mongodb.operation.AggregateOperation$1.call(AggregateOperation.java:253)   at com.mongodb.operation.OperationHelper.withConnectionSource(OperationHelper.java:431)   at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:404)   at com.mongodb.operation.AggregateOperation.execute(AggregateOperation.java:253)   at com.mongodb.operation.AggregateOperation.execute(AggregateOperation.java:67)   at com.mongodb.Mongo.execute(Mongo.java:836)   at com.mongodb.Mongo$2.execute(Mongo.java:823)   at com.mongodb.DBCollection.aggregate(DBCollection.java:1455)   at com.mongodb.DBCollection.aggregate(DBCollection.java:1380)   at com.mongodb.DBCollection.aggregate(DBCollection.java:1366)   at com.mycompany.test.Main.main(Main.java:200)      \"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Memory leaks when using NettyStream and CommandListener\"","issue_description":"\"I found some strange behaviour when NettyStream is used in async driver with a CommandListener configured. See next code example: https:\/\/gist.github.com\/<USER>957aab95a8385ef43768f378fb405d25    There I enabled high level of Netty resource leak detector to see any problems with resource leakage. When line {code}.addCommandListener(new NopCommandListener()){code} is commented all works just fine (see normal_run.log). But when that line is uncommented resource leak detector found some leaks (see buggy_run.log).    After playing with this bug a little bit I found that root cause of these leaks is  [ByteBufBsonDocument|https:\/\/github.com\/mongodb\/mongo-java-driver\/blob\/master\/driver-core\/src\/main\/com\/mongodb\/connection\/ByteBufBsonDocument.java] class. When instance of this class is [created|https:\/\/github.com\/mongodb\/mongo-java-driver\/blob\/94780bc8b72c62d9bc09beaa9ac62b942debab5f\/driver-core\/src\/main\/com\/mongodb\/connection\/CommandProtocol.java#L190-L191] it takes ownership of given buffer (because [bsonOutput.getByteBuffers()|https:\/\/github.com\/mongodb\/mongo-java-driver\/blob\/79a6e9366139a40129015c041f1bc25aad784343\/driver-core\/src\/main\/com\/mongodb\/connection\/ByteBufBsonDocument.java#L78] increments reference counter in each returning sub buffer). Unfortunately ByteBufBsonDocument class never releases its buffer and currently there is no way to do that.\"","issue_type":"Bug","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Spurious exception in maintenance task\"","issue_description":"\"I use version 3.4.1 and get the following exception during restart of the application:  exception thrown during connection pool background maintenance task  {code}  java.lang.InterruptedException: null   at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(Unknown Source)   at java.util.concurrent.Semaphore.acquire(Unknown Source)   at com.mongodb.internal.connection.ConcurrentPool.acquirePermit(ConcurrentPool.java:182)   ... 38 common frames omitted  Wrapped by: com.mongodb.MongoInterruptedException: Interrupted acquiring a permit to retrieve an item from the pool    at com.mongodb.internal.connection.ConcurrentPool.acquirePermit(ConcurrentPool.java:186)   at com.mongodb.internal.connection.ConcurrentPool.get(ConcurrentPool.java:126)   at com.mongodb.internal.connection.ConcurrentPool.get(ConcurrentPool.java:109)   at com.mongodb.internal.connection.PowerOfTwoBufferPool.getBuffer(PowerOfTwoBufferPool.java:76)   at com.mongodb.connection.SocketStream.getBuffer(SocketStream.java:69)   at com.mongodb.connection.InternalStreamConnection.getBuffer(InternalStreamConnection.java:514)   at com.mongodb.connection.ByteBufferBsonOutput.getByteBufferAtIndex(ByteBufferBsonOutput.java:91)   at com.mongodb.connection.ByteBufferBsonOutput.getCurrentByteBuffer(ByteBufferBsonOutput.java:80)   at com.mongodb.connection.ByteBufferBsonOutput.writeByte(ByteBufferBsonOutput.java:75)   at org.bson.io.OutputBuffer.write(OutputBuffer.java:150)   at org.bson.io.OutputBuffer.writeInt32(OutputBuffer.java:56)   at com.mongodb.connection.RequestMessage.writeMessagePrologue(RequestMessage.java:171)   at com.mongodb.connection.RequestMessage.encodeWithMetadata(RequestMessage.java:159)   at com.mongodb.connection.RequestMessage.encode(RequestMessage.java:147)   at com.mongodb.connection.CommandHelper.sendMessage(CommandHelper.java:88)   at com.mongodb.connection.CommandHelper.executeCommand(CommandHelper.java:32)   at com.mongodb.connection.SaslAuthenticator.sendSaslContinue(SaslAuthenticator.java:121)   at com.mongodb.connection.SaslAuthenticator.access$100(SaslAuthenticator.java:37)   at com.mongodb.connection.SaslAuthenticator$1.run(SaslAuthenticator.java:63)   ... 20 common frames omitted  Wrapped by: com.mongodb.MongoSecurityException: Exception authenticating MongoCredential{mechanism=null, userName='composing', source='composing', password=<hidden>, mechanismProperties={}}   at com.mongodb.connection.SaslAuthenticator.wrapInMongoSecurityException(SaslAuthenticator.java:157)   at com.mongodb.connection.SaslAuthenticator.access$200(SaslAuthenticator.java:37)   at com.mongodb.connection.SaslAuthenticator$1.run(SaslAuthenticator.java:66)   at com.mongodb.connection.SaslAuthenticator$1.run(SaslAuthenticator.java:44)   at com.mongodb.connection.SaslAuthenticator.doAsSubject(SaslAuthenticator.java:162)   at com.mongodb.connection.SaslAuthenticator.authenticate(SaslAuthenticator.java:44)   at com.mongodb.connection.DefaultAuthenticator.authenticate(DefaultAuthenticator.java:32)   at com.mongodb.connection.InternalStreamConnectionInitializer.authenticateAll(InternalStreamConnectionInitializer.java:109)   at com.mongodb.connection.InternalStreamConnectionInitializer.initialize(InternalStreamConnectionInitializer.java:46)   at com.mongodb.connection.InternalStreamConnection.open(InternalStreamConnection.java:116)   at com.mongodb.connection.UsageTrackingInternalConnection.open(UsageTrackingInternalConnection.java:47)   at com.mongodb.connection.DefaultConnectionPool$UsageTrackingInternalConnectionItemFactory.create(DefaultConnectionPool.java:494)   at com.mongodb.connection.DefaultConnectionPool$UsageTrackingInternalConnectionItemFactory.create(DefaultConnectionPool.java:482)   at com.mongodb.internal.connection.ConcurrentPool.createNewAndReleasePermitIfFailure(ConcurrentPool.java:164)   at com.mongodb.internal.connection.ConcurrentPool.ensureMinSize(ConcurrentPool.java:158)   at com.mongodb.connection.DefaultConnectionPool$3.run(DefaultConnectionPool.java:302)   at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)   at java.util.concurrent.FutureTask.runAndReset(Unknown Source)   at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)   at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)   at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)   at java.lang.Thread.run(Unknown Source)  Exception thrown during connection pool background maintenance task  {code}    There exists already the issue JAVA-2123. But the solution does not seem to work here. In that solution the provided bugfix catched the MongoInteruptedException. But in this exception the MongoInteruptedException gets wrapped inside a securityException. Thats why the catch block in the defaultConnectionPool is not working.\"","issue_type":"Bug","issue_priority":"Minor - P4","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Make driver-async an OSGI module\"","issue_description":"\"The driver-async module can be an OSGI module, but currently it's not.\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Deprecate modifiers in FindOptions and replace with properties\"","issue_description":"","issue_type":"New Feature","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Support DBRef codec in async driver (MongoClients.DEFAULT_CODEC_REGISTRY)\"","issue_description":"\"Storing {{DBRef}} with the RxJava\/ReactiveStreams driver fails because no codec can be found for {{DBRef}}. Currently, the async driver {{CodecRegistry}} is configured with:    {code}  private static final CodecRegistry DEFAULT_CODEC_REGISTRY =              fromProviders(asList(new ValueCodecProvider(),                      new DocumentCodecProvider(),                      new BsonValueCodecProvider(),                      new IterableCodecProvider(),                      new GeoJsonCodecProvider(),                      new GridFSFileCodecProvider()));  {code}    Please add the {{DBRefCodec}} and the DBRef <-> Document transformation so references can be used from the asynchronous\/reactive drivers.\"","issue_type":"New Feature","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"$set fails when setting sub-document as a Map\"","issue_description":"\"If you try to use $set to update a sub-document as a Map, you get:    {noformat}  Exception in thread \"\"main\"\" org.bson.codecs.configuration.CodecConfigurationException: Can't find a codec for class java.util.HashMap.   at org.bson.codecs.configuration.CodecCache.getOrThrow(CodecCache.java:46)   at org.bson.codecs.configuration.ProvidersCodecRegistry.get(ProvidersCodecRegistry.java:63)   at org.bson.codecs.configuration.ProvidersCodecRegistry.get(ProvidersCodecRegistry.java:37)   at com.mongodb.client.model.BuildersHelper.encodeValue(BuildersHelper.java:35)   at com.mongodb.client.model.Updates$SimpleUpdate.toBsonDocument(Updates.java:442)   at com.mongodb.MongoCollectionImpl.toBsonDocument(MongoCollectionImpl.java:599)   at com.mongodb.MongoCollectionImpl.update(MongoCollectionImpl.java:542)   at com.mongodb.MongoCollectionImpl.updateOne(MongoCollectionImpl.java:381)   at com.mongodb.MongoCollectionImpl.updateOne(MongoCollectionImpl.java:376)  {noformat}    The attached project will reproduce the failure.\"","issue_type":"Bug","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Reduce memory requirements for Document and BsonDocument when the number of keys is small\"","issue_description":"\"Both Document and BsonDocument have an internal LinkedHashMap to store the entries in order.    This is not the most efficient data structure to use when the number of entries is small.      Consider using just an ArrayList to handle small numbers of entries, and only start using a hash table when the number of entries grows to more than a handful (size of handful TBD).\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Unclear API\"","issue_description":"\"I was under the impression that catching `MongoException` was sufficient for error handling driver calls to the database. In cases of network\/mongo instability, I'm also seeing `IllegalStateExceptions` which is escaping our normal error handling.   Would it make sense to have all exceptions extend from `MongoException`? If not, is it possible to publicly document a definitive list of high-level exceptions that can come out of mongo driver calls that all applications are recommended to catch? Is that list limited to `MongoException` and `IllegalArgumentException`? If so, can we guarantee nothing new will be added that doesn't fit that classification or is it recommended for applications to catch `Exception`?  {noformat}      [java] Exception in thread \"\"Thread-47745\"\" java.lang.IllegalStateException: open      [java]  at org.bson.util.Assertions.isTrue(Assertions.java:36)      [java]  at com.mongodb.DBTCPConnector.releasePort(DBTCPConnector.java:413)      [java]  at com.mongodb.DBCollectionImpl.insert(DBCollectionImpl.java:198)      [java]  at com.mongodb.DBCollectionImpl.insert(DBCollectionImpl.java:167)      [java]  at com.mongodb.DBCollection.insert(DBCollection.java:161)      [java]  at com.xgen.svc.brs.dao.BlockStoreDao.saveBlocks(BlockStoreDao.java:234) ... {noformat}\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"JVM Sprint 39","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Make RawBsonDocument#get return RawBsonDocument instances\"","issue_description":"\"Currently RawBsonDocument.get(Object key), when returning a BsonDocument, fulling decodes it.  A more efficient way would be to return a RawBsonDocument pointing to just the raw bytes, which may or may not be a slice of the bytes managed by the container.\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Mesosphere Sprint 2018-24","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Support systemd and freezer cgroup subsystems bind mount for container with rootfs.\"","issue_description":"\"From MESOS-8327, cgroup subsystems are bind mounted to the container's rootfs, but systemd and freezer cgroup are not bind mounted yet since they are not subsystems under the cgroup isolator but from the linux launcher.    Some applications (e.g., dockerd) may check the \/proc\/self\/cgorup for enabled subsystems and check them at \/proc\/self\/mountinfo to make sure there are those mounts. Here is an example:  {noformat}  \u279c  aws  dcos task exec --interactive test.bf2fad80-846b-11e8-b5a0-eaa1bec34306 \/bin\/bash  cat \/proc\/self\/cgroup  11:blkio:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543  10:perf_event:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543  9:cpuset:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543  8:memory:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543  7:pids:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543  6:devices:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543  5:cpu,cpuacct:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543  4:freezer:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543\/mesos\/12fde554-5262-473c-a20c-7dd201148b11  3:net_cls,net_prio:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543  2:hugetlb:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543  1:name=systemd:\/mesos\/87899f08-53e5-47bf-aba3-712c31c33543\/mesos\/12fde554-5262-473c-a20c-7dd201148b11                        cat \/proc\/self\/mountinfo  388 387 202:9 \/ \/ rw,relatime master:1 - ext4 \/dev\/xvda9 rw,seclabel,data=ordered  389 388 254:0 \/ \/usr ro,relatime master:2 - ext4 \/dev\/mapper\/usr ro,seclabel,block_validity,delalloc,barrier,user_xattr,acl  390 389 202:6 \/ \/usr\/share\/oem rw,nodev,relatime master:32 - ext4 \/dev\/xvda6 rw,seclabel,commit=600,data=ordered  391 388 0:6 \/ \/dev rw,nosuid master:3 - devtmpfs devtmpfs rw,seclabel,size=8201844k,nr_inodes=2050461,mode=755  392 391 0:19 \/ \/dev\/shm rw,nosuid,nodev master:4 - tmpfs tmpfs rw,seclabel  393 391 0:20 \/ \/dev\/pts rw,nosuid,noexec,relatime master:5 - devpts devpts rw,seclabel,gid=5,mode=620,ptmxmode=000  394 391 0:15 \/ \/dev\/mqueue rw,relatime master:26 - mqueue mqueue rw,seclabel  395 391 0:37 \/ \/dev\/hugepages rw,relatime master:27 - hugetlbfs hugetlbfs rw,seclabel  396 388 0:4 \/ \/proc rw,nosuid,nodev,noexec,relatime master:6 - proc proc rw  397 396 0:35 \/ \/proc\/sys\/fs\/binfmt_misc rw,relatime master:24 - autofs systemd-1 rw,fd=23,pgrp=0,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=1017  398 396 0:40 \/ \/proc\/xen rw,relatime master:31 - xenfs xenfs rw  399 388 0:18 \/ \/sys rw,nosuid,nodev,noexec,relatime master:7 - sysfs sysfs rw,seclabel  400 399 0:17 \/ \/sys\/kernel\/security rw,nosuid,nodev,noexec,relatime master:8 - securityfs securityfs rw  401 399 0:22 \/ \/sys\/fs\/cgroup ro,nosuid,nodev,noexec master:9 - tmpfs tmpfs ro,seclabel,mode=755  402 401 0:23 \/ \/sys\/fs\/cgroup\/systemd rw,nosuid,nodev,noexec,relatime master:10 - cgroup cgroup rw,xattr,release_agent=\/usr\/lib\/systemd\/systemd-cgroups-agent,name=systemd  403 401 0:25 \/ \/sys\/fs\/cgroup\/hugetlb rw,nosuid,nodev,noexec,relatime master:11 - cgroup cgroup rw,hugetlb  404 401 0:26 \/ \/sys\/fs\/cgroup\/net_cls,net_prio rw,nosuid,nodev,noexec,relatime master:12 - cgroup cgroup rw,net_cls,net_prio  405 401 0:27 \/ \/sys\/fs\/cgroup\/freezer rw,nosuid,nodev,noexec,relatime master:13 - cgroup cgroup rw,freezer  406 401 0:28 \/ \/sys\/fs\/cgroup\/cpu,cpuacct rw,nosuid,nodev,noexec,relatime master:14 - cgroup cgroup rw,cpu,cpuacct  407 401 0:29 \/ \/sys\/fs\/cgroup\/devices rw,nosuid,nodev,noexec,relatime master:15 - cgroup cgroup rw,devices  408 401 0:30 \/ \/sys\/fs\/cgroup\/pids rw,nosuid,nodev,noexec,relatime master:16 - cgroup cgroup rw,pids  409 401 0:31 \/ \/sys\/fs\/cgroup\/memory rw,nosuid,nodev,noexec,relatime master:17 - cgroup cgroup rw,memory  410 401 0:32 \/ \/sys\/fs\/cgroup\/cpuset rw,nosuid,nodev,noexec,relatime master:18 - cgroup cgroup rw,cpuset  411 401 0:33 \/ \/sys\/fs\/cgroup\/perf_event rw,nosuid,nodev,noexec,relatime master:19 - cgroup cgroup rw,perf_event  412 401 0:34 \/ \/sys\/fs\/cgroup\/blkio rw,nosuid,nodev,noexec,relatime master:20 - cgroup cgroup rw,blkio  413 399 0:24 \/ \/sys\/fs\/pstore rw,nosuid,nodev,noexec,relatime master:21 - pstore pstore rw,seclabel  414 399 0:16 \/ \/sys\/fs\/selinux rw,relatime master:22 - selinuxfs selinuxfs rw  415 399 0:7 \/ \/sys\/kernel\/debug rw,relatime master:29 - debugfs debugfs rw,seclabel  416 388 0:21 \/ \/run rw,nosuid,nodev master:23 - tmpfs tmpfs rw,seclabel,mode=755  417 388 0:36 \/ \/boot rw,relatime master:25 - autofs systemd-1 rw,fd=33,pgrp=0,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=10774  418 417 202:1 \/ \/boot rw,relatime master:33 - vfat \/dev\/xvda1 rw,fmask=0022,dmask=0022,codepage=437,iocharset=ascii,shortname=mixed,errors=remount-ro  419 388 0:38 \/ \/media rw,nosuid,nodev,noexec,relatime master:28 - tmpfs tmpfs rw,seclabel  420 388 0:39 \/ \/tmp rw,nosuid,nodev master:30 - tmpfs tmpfs rw,seclabel  421 388 202:16 \/ \/var\/lib rw,relatime master:218 - ext4 \/dev\/xvdb rw,seclabel,data=ordered  422 421 202:16 \/docker\/overlay \/var\/lib\/docker\/overlay rw,relatime - ext4 \/dev\/xvdb rw,seclabel,data=ordered  423 421 202:16 \/mesos\/slave\/volumes\/roles\/kubernetes-role\/b12a0508-c837-4d89-b1e3-d1400355833c \/var\/lib\/mesos\/slave\/slaves\/cbb0007d-bcc7-4fe8-b47d-3d67604a2eb2-S0\/frameworks\/cbb0007d-bcc7-4fe8-b47d-3d67604a2eb2-0002\/executors\/kubernetes__etcd__465602c0-ad54-4f46-960e-3a5e8e18f3e8\/runs\/300d07e7-319d-4642-b9c9-63b9293765fd\/data-dir rw,relatime master:218 - ext4 \/dev\/xvdb rw,seclabel,data=ordered  424 421 202:16 \/mesos\/slave\/volumes\/roles\/kubernetes-role\/a60b4165-e5ee-4847-8437-2a7f78f38c5d \/var\/lib\/mesos\/slave\/slaves\/cbb0007d-bcc7-4fe8-b47d-3d67604a2eb2-S0\/frameworks\/cbb0007d-bcc7-4fe8-b47d-3d67604a2eb2-0002\/executors\/kubernetes__etcd__465602c0-ad54-4f46-960e-3a5e8e18f3e8\/runs\/300d07e7-319d-4642-b9c9-63b9293765fd\/wal-pv rw,relatime master:218 - ext4 \/dev\/xvdb rw,seclabel,data=ordered  426 396 0:51 \/ \/proc rw,nosuid,nodev,noexec,relatime - proc proc rw  427 421 0:52 \/ \/var\/lib\/mesos\/slave\/slaves\/cbb0007d-bcc7-4fe8-b47d-3d67604a2eb2-S0\/frameworks\/cbb0007d-bcc7-4fe8-b47d-3d67604a2eb2-0001\/executors\/test.bf2fad80-846b-11e8-b5a0-eaa1bec34306\/runs\/87899f08-53e5-47bf-aba3-712c31c33543\/.secret-113d83da-d9ce-4a5f-9565-9179ed8bd94a rw,relatime - ramfs ramfs rw      \u279c  aws  dcos task exec --interactive debian.6c333651-846c-11e8-b5a0-eaa1bec34306 \/bin\/bash  cat \/proc\/self\/cgroup  11:freezer:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc\/mesos\/e69b6a82-4c4a-4758-99c8-6afac41ae1a5  10:devices:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc  9:hugetlb:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc  8:blkio:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc  7:cpuset:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc  6:pids:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc  5:perf_event:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc  4:cpu,cpuacct:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc  3:memory:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc  2:net_cls,net_prio:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc  1:name=systemd:\/mesos\/66896178-3726-439f-ac45-6eb025b944fc\/mesos\/e69b6a82-4c4a-4758-99c8-6afac41ae1a5    cat \/proc\/self\/mountinfo  466 423 0:51 \/ \/ rw,relatime master:148 - overlay overlay rw,lowerdir=\/tmp\/xRzx5s\/1:\/tmp\/xRzx5s\/0,upperdir=\/var\/lib\/mesos\/slave\/provisioner\/containers\/66896178-3726-439f-ac45-6eb025b944fc\/backends\/overlay\/scratch\/704eebdc-1862-4054-9245-2025563a1919\/upperdir,workdir=\/var\/lib\/mesos\/slave\/provisioner\/containers\/66896178-3726-439f-ac45-6eb025b944fc\/backends\/overlay\/scratch\/704eebdc-1862-4054-9245-2025563a1919\/workdir  467 466 202:9 \/etc\/resolv.conf\/\/deleted \/etc\/resolv.conf ro,nosuid,nodev,noexec,relatime master:1 - ext4 \/dev\/xvda9 rw,seclabel,data=ordered  468 466 202:9 \/etc\/hostname \/etc\/hostname ro,nosuid,nodev,noexec,relatime master:1 - ext4 \/dev\/xvda9 rw,seclabel,data=ordered  469 466 202:9 \/etc\/hosts \/etc\/hosts ro,nosuid,nodev,noexec,relatime master:1 - ext4 \/dev\/xvda9 rw,seclabel,data=ordered  470 466 202:16 \/mesos\/slave\/slaves\/cbb0007d-bcc7-4fe8-b47d-3d67604a2eb2-S1\/frameworks\/cbb0007d-bcc7-4fe8-b47d-3d67604a2eb2-0001\/executors\/debian.6c333651-846c-11e8-b5a0-eaa1bec34306\/runs\/66896178-3726-439f-ac45-6eb025b944fc \/mnt\/mesos\/sandbox rw,relatime master:218 - ext4 \/dev\/xvdb rw,seclabel,data=ordered  471 466 0:52 \/ \/proc rw,nosuid,nodev,noexec,relatime - proc proc rw  472 471 0:52 \/bus \/proc\/bus ro,nosuid,nodev,noexec,relatime - proc proc rw  473 471 0:52 \/fs \/proc\/fs ro,nosuid,nodev,noexec,relatime - proc proc rw  474 471 0:52 \/irq \/proc\/irq ro,nosuid,nodev,noexec,relatime - proc proc rw  475 471 0:52 \/sys \/proc\/sys ro,nosuid,nodev,noexec,relatime - proc proc rw  476 471 0:52 \/sysrq-trigger \/proc\/sysrq-trigger ro,nosuid,nodev,noexec,relatime - proc proc rw  477 466 0:18 \/ \/sys ro,nosuid,nodev,noexec,relatime - sysfs sysfs rw,seclabel  478 477 0:54 \/ \/sys\/fs\/cgroup rw,nosuid,nodev,noexec,relatime - tmpfs tmpfs rw,seclabel,mode=755  479 466 0:55 \/ \/dev rw,nosuid,noexec - tmpfs tmpfs rw,seclabel,mode=755  480 479 0:56 \/ \/dev\/pts rw,nosuid,noexec,relatime - devpts devpts rw,seclabel,mode=600,ptmxmode=666  481 479 0:57 \/ \/dev\/shm rw,nosuid,nodev - tmpfs tmpfs rw,seclabel  482 478 0:31 \/mesos\/66896178-3726-439f-ac45-6eb025b944fc \/sys\/fs\/cgroup\/blkio rw,nosuid,nodev,noexec,relatime master:17 - cgroup cgroup rw,blkio  483 478 0:27 \/mesos\/66896178-3726-439f-ac45-6eb025b944fc \/sys\/fs\/cgroup\/cpu,cpuacct rw,nosuid,nodev,noexec,relatime master:13 - cgroup cgroup rw,cpu,cpuacct  484 478 0:30 \/mesos\/66896178-3726-439f-ac45-6eb025b944fc \/sys\/fs\/cgroup\/cpuset rw,nosuid,nodev,noexec,relatime master:16 - cgroup cgroup rw,cpuset  485 478 0:33 \/mesos\/66896178-3726-439f-ac45-6eb025b944fc \/sys\/fs\/cgroup\/devices rw,nosuid,nodev,noexec,relatime master:19 - cgroup cgroup rw,devices  486 478 0:32 \/mesos\/66896178-3726-439f-ac45-6eb025b944fc \/sys\/fs\/cgroup\/hugetlb rw,nosuid,nodev,noexec,relatime master:18 - cgroup cgroup rw,hugetlb  487 478 0:26 \/mesos\/66896178-3726-439f-ac45-6eb025b944fc \/sys\/fs\/cgroup\/memory rw,nosuid,nodev,noexec,relatime master:12 - cgroup cgroup rw,memory  488 478 0:25 \/mesos\/66896178-3726-439f-ac45-6eb025b944fc \/sys\/fs\/cgroup\/net_cls,net_prio rw,nosuid,nodev,noexec,relatime master:11 - cgroup cgroup rw,net_cls,net_prio  489 478 0:28 \/mesos\/66896178-3726-439f-ac45-6eb025b944fc \/sys\/fs\/cgroup\/perf_event rw,nosuid,nodev,noexec,relatime master:14 - cgroup cgroup rw,perf_event  490 478 0:29 \/mesos\/66896178-3726-439f-ac45-6eb025b944fc \/sys\/fs\/cgroup\/pids rw,nosuid,nodev,noexec,relatime master:15 - cgroup cgroup rw,pids  {noformat}    The first one is a task without image, the second one is a task using debian image. So any app relies on systemd and freezer cgroup would may fail:  {noformat}  returned error: cgroups: cannot find cgroup mount destination: unknown .\/docker\/docker: Error response from daemon: cgroups: cannot find cgroup mount destination: unknown.  {noformat}    So, we should consider add systemd and freezer cgroup bind mount at the cgroup isolator and make a *NOTE* for this behavior.\"","issue_type":"Task","issue_priority":"Major","story_points":3.0}
{"sprint_name":"Mesosphere Sprint 2018-24","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Changing `CREATE_VOLUME` and `CREATE_BLOCK` to `CREATE_DISK`.\"","issue_description":"\"Mesos 1.5 introduced four new operations for better storage support through CSI. These operations are:      * CREATE_VOLUME converts RAW disks to MOUNT or PATH disks.    * DESTROY_VOLUME converts MOUNT or PATH disks back to RAW disks.    * CREATE_BLOCK converts RAW disks to BLOCK disks.    * DESTROY_BLOCK converts BLOCK disks back to RAW disks.    However, the following two issues are raised for these operations:    1. \"\"Volume\"\" is overloaded and leads to conflicting\/inconsistent naming.  2. The concept of \"\"PATH\"\" disks does not exist in CSI, which could be problematic.    To address this, we could change CREATE_VOLUME\/CREATE_BLOCK to CREATE_DISK, and DESTROY_VOLUME\/DESTROY_BLOCK to DESTROY_DISK, and make CREATE_DISK support only MOUNT and BLOCK disks.\"","issue_type":"Task","issue_priority":"Major","story_points":3.0}
{"sprint_name":"Mesosphere Sprint 2018-24","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Make gRPC call deadline configurable.\"","issue_description":"\"Currently, the deadline for a gRPC call to become terminal is hard-coded to 5 seconds. This would cause problems on slow machines. Ideally, we should make this deadline configurable.\"","issue_type":"Improvement","issue_priority":"Major","story_points":3.0}
{"sprint_name":"Mesosphere Sprint 2018-24","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Improve the container preparing logging in IOSwitchboard and volume\/secret isolator.\"","issue_description":"\"Improve the container preparing logging in IOSwitchboard and volume\/secret isolator.\"","issue_type":"Improvement","issue_priority":"Major","story_points":2.0}
{"sprint_name":"Mesosphere Sprint 2018-24","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add GC capability to nested containers\"","issue_description":"\"We should extend the existing API or add a new API for nested containers for an executor to tell the Mesos agent that a nested container is no longer needed and can be scheduled for GC.\"","issue_type":"Improvement","issue_priority":"Major","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"replace testsource with time and testsink with log\"","issue_description":"\"This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing (should accept --interval for the seconds between time messages).\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"add spring-integration-groovy to container dependencies\"","issue_description":"\"This will enable the use of groovy scripts within modules.\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"add SpEL 'filter' processor\"","issue_description":"\"It should provide an 'expression' param for SpEL and have a default value of true (accept everything).\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"add SpEL 'transform' processor\"","issue_description":"\"It should provide an 'expression' param for SpEL and have a default pass-thru of the payload.\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add install script for Redis\"","issue_description":"\"This assumes the redis source tar is available under $rootDir\/redis\/redis-2.6.13.tar.gz  The install script does the following:  - Check the platform OS & arch - unzip the tar, compile the sources\"","issue_type":"Technical task","issue_priority":"Minor","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update XD to Use SI 3.0.0.M2\"","issue_description":"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Create final distribution zip across multiple projects\"","issue_description":"\"The final directory structure should look like  <install-dir>\/xd <install-dir>\/redis <install-dir>\/gemfire  inside the XD directory   \/xd\/bin - which has xd-container and xd-admin scripts \/xd\/lib  inside the gemfire directory \/gemfire\/bin - has the gemfire-server script \/gemfire\/lib   inside the redis directory is   \/redis\/redis-latest-v.x.y.z.tar \/redis\/README \/readis\/install-redis  - script that does the basic 4 commands to install redis.   There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and 'redis-binary' directories and creates the final layout for the distribution.\"","issue_type":"Story","issue_priority":"Minor","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for starting Spring XD servers\"","issue_description":"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Build script should not package 'spring-xd-dirt' scripts \"","issue_description":"\"We are packaging separate scripts to start XDAdmin and XDContainer.  The Gradle application plugin will generate an unwanted 'spring-xd-dirt' scripts, this should be removed from the bin directory when creating a distribution zip.\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add LICENSE to be included in root directory of distribution\"","issue_description":"\"should contain apache licence\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add README to be included in root directory of distribution\"","issue_description":"\"should explain basic layout of the distribution\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Create XDAdmin server to start container launcher\"","issue_description":"\"This will launch the RedisContainerLauncher, in future will be able to select from a variety of middleware options.\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Create XDContainer class to start stream server\"","issue_description":"\"Provide optional command line arg to embed the container launcher, aka - xd-admin server.    XDContainer.sh --embeddAdmin\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add gradle tasks that build and bundle the redis server\"","issue_description":"","issue_type":"Technical task","issue_priority":"Minor","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Sonar build is failing\"","issue_description":"\"https:\/\/build.springsource.org\/browse\/XD-SONAR-34  Caused by: java.lang.ClassNotFoundException: org.sonar.api.Plugin         at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass(SelfFirstStrategy.java:50)         at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:244)         at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:230)         ... 94 more\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation on the module system and how to contribute new modules\"","issue_description":"\"For people who are familiar with Spring\/Spring Integration provide documents that show how to add additional input sources\/sinks.\"","issue_type":"Story","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for field value taps\"","issue_description":"\"Put on the guide as a section in an 'input-stream' wiki page.   \"","issue_type":"Story","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for counter taps\"","issue_description":"\"Put on the guide as a section in an 'input-stream' wiki page.   \"","issue_type":"Story","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for \"\"twittersearch | file\"\" processing\"","issue_description":"\"Put on the guide as a section in an 'input-stream' wiki page.   \"","issue_type":"Story","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for \"\"gemfirecq | file\"\" processing\"","issue_description":"\"Put on the guide as a section in an 'input-stream' wiki page.   \"","issue_type":"Story","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for \"\"http | gemfire\"\" processing\"","issue_description":"\"Put on the guide as a section in an 'input-stream' wiki page.   \"","issue_type":"Story","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for \"\"tail | file\"\" processing\"","issue_description":"\"Put on the guide as a section in an 'input-stream' wiki page.   \"","issue_type":"Story","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for \"\"http | hdfs\"\" processing\"","issue_description":"\"Put on the guide as a section in an 'input-stream' wiki page.   \"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"End user guide for data streams\"","issue_description":"\"Put on the guide as a section in an 'streams' wiki page.  End user focused, no need to mention spring underpinning, impl details.  \"","issue_type":"Story","issue_priority":"Minor","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for \"\"http | file\"\" processing\"","issue_description":"\"Put on the guide as a section in an 'input sources' wiki page.  https:\/\/github.com\/springsource\/spring-xd\/wiki\/GuideGettingStarted  \"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add gemfire-server application to the distribution zip of the project spring-xd-gemfire-server\"","issue_description":"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add redis bundle to distribution zip file\"","issue_description":"\"for linux and mac\"","issue_type":"Epic","issue_priority":"Minor","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Create XD module for tail file adapter\"","issue_description":"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Provide a http source\"","issue_description":"\"stream should be able to ingest data from http \"","issue_type":"Story","issue_priority":"Minor","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Tuple should support storing nested tuples\"","issue_description":"\"Nested tuple structures shoudl be supported,  getTuple(int index), getTuple(String name)\"","issue_type":"Story","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Entity validation fails and throws a IndexOutOfBoundsException\"","issue_description":"\"There is a problem in class DefaultCassandraPersistentEntityMetadataVerifier. The table verification throws an IndexOutOfBoundsException in line 214 if less than one id property exists.  \"","issue_type":"New Feature","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Integrate CqlIdentifier\"","issue_description":"","issue_type":"New Feature","issue_priority":"Major","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add class AbstractCqlTemplateConfiguration\"","issue_description":"\"Add class AbstractCqlTemplateConfiguration extends AbstractSessionConfiguration that defines a \"\"@Bean CqlOperations cqlTemplate()\"\" method.\"","issue_type":"New Feature","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Enhance annotations to allow for force-quoting\"","issue_description":"\"Any annotation that allows the user to specify a CQL identifier (table, column or index name) needs to be enhanced to include an optional flag, called something like \"\"forceQuoteName\"\", that enables a user to cause the identifier to be force-quoted.  The flag should be false by default.\"","issue_type":"New Feature","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add support for missing DataTypes to DefaultCassandraRowValueProvider\"","issue_description":"\"Currently there is only support for columes of types text, cint and bytes in org.springframework.data.cassandra.convert.DefaultCassandraRowValueProvider. The missing types from DataType should be added.  To reproduce try to read a column of type boolean from cassandra. In this case the column falls through and it is handled as bytes. In the handler for bytes an assertion fails, because datatypes are wrong (boolean != bytes). This should be reproducible for all datatypes except text, cint and bytes.\"","issue_type":"Bug","issue_priority":"Major","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Ensure all CqlOperations methods take QueryOptions where appropriate\"","issue_description":"\"I noticed that there is no overload of \"\"void execute(String)\"\" that takes a QueryOptions object, like \"\"void execute(String, QueryOptions)\"\".  This request is to review all CqlOperations methods to ensure that we're accepting a QueryOptions object wherever appropriate.\"","issue_type":"New Feature","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add overloaded methods to CqlOperations that use strongly typed Query objects\"","issue_description":"\"For each method on CqlOperations that takes a CQL string, there should be an equivalent overload that takes a Query object (or Select, Update, Insert, Delete, etc, as appropriate, depending on the operation).  I already overloaded \"\"void execute(String)\"\" with \"\"void execute(Query)\"\" in the DATACASS-33 branch, but there are many others.\"","issue_type":"New Feature","issue_priority":"Minor","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Ensure license text appears in all source files with proper dates.\"","issue_description":"\"The Apache open source license header needs to appear in all source files with the appropriate date range.\"","issue_type":"New Feature","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"xml:base URL embedded in yum metadata breaks proxy repositories\"","issue_description":"\"In fixing NEXUS-5806 we started to embed the server base URL into the xml:base of the yum primary.xml.gz file.    This causes proxy's of yum enabled group repositories to fail, requests to download artifacts from yum are made to the proxy's remote instead of the proxy itself.\"","issue_type":"Bug","issue_priority":"Major","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"review File.mkdirs() usage, replace with Files.createDirectory(file.toPath()); to not hide IOExceptions\"","issue_description":"\"see summary - we don't want to hide low level IOExceptions anymore in buggy code\"","issue_type":"Improvement","issue_priority":"Major","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"If \"\"application server settings (optional)\"\" is not checked than administration\/server page can't be saved.\"","issue_description":"\"Uncheck \"\"application server settings (optional)\"\", under \"\"administration\/server\"\" and save.    Now make another change on the page and attempt to save.  This fails.    Why this section is optional anyhow?\"","issue_type":"Task","issue_priority":"Major","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Remove pgp.mit.edu from list of uses SKS Keyservers\"","issue_description":"\"nexus-pgp-plugin uses a list of SKS servers to perform signature validation (used by Staging and Procurement plugins).    The default entries are:  * http:\/\/pool.sks-keyservers.net:11371\/  * http:\/\/pgp.mit.edu:11371\/    While the first itself is a _SKS server pool_, the latter is a \"\"simple standalone\"\" one. Moreover, the latter one _was removed from the former's pool_ as it's not maintained. It uses SKS Server version 1.1.1 while the pool is on version 1.1.3, 1.1.4 and above (1.1.4+).    With latest changes (NEXUS-5969 for example), the MIT SKS server will just create log pollution, as for example GPG subkey search is added and works only in SKS Server version 1.1.3 and above    https:\/\/bitbucket.org\/skskeyserver\/sks-keyserver\/src\/4069c369eaaa718c6d4f19427f8f164fb9a1e1f0\/CHANGELOG?at=default#cl-65    Hence, even if the check will succeed, log will contain spam if MIT server is tried first (and we randomly choose servers from the list AFAIK).    Proposed solution: remove MIT from default configuration    More about this: The hostname \"\"pool.sks-keyservers.net\"\" will be resolved to an actual server based on requester location and other factors. Problem with retries (ie. you got assigned a server that is 0xdeadbeef, unlikely but still possible), is that OS and Java will cache DNS resolution result. A \"\"real retry\"\" would be to resolve that hostname again, and retry with new IP.  \"","issue_type":"Improvement","issue_priority":"Trivial","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"remove sonatype-indexer use of _magic_ annotations\"","issue_description":"\"A few components use magic to imply jsr-330 annotations.  This should be fixed by putting the proper annotation on the component.    {quote}  jvm 1    | 2013-10-04 10:26:44 WARN  [jetty-main-thread-1]  org.sonatype.guice.nexus.scanners.NexusTypeVisitor - Found legacy @org.sonatype.plugin.Managed annotation: com.sonatype.index.api.SearcherRegistry  jvm 1    | 2013-10-04 10:26:44 WARN  [jetty-main-thread-1]  org.sonatype.guice.nexus.scanners.NexusTypeVisitor - Found legacy @org.sonatype.plugin.Managed annotation: com.sonatype.index.api.IndexerRegistry  jvm 1    | 2013-10-04 10:26:44 WARN  [jetty-main-thread-1]  org.sonatype.guice.nexus.scanners.NexusTypeVisitor - Found legacy component relying on @org.sonatype.plugin.Managed magic to automatically imply @javax.inject.Named @javax.inject.Singleton: com.sonatype.index.rdf.internal.DefaultRdfIndexFactory  jvm 1    | 2013-10-04 10:26:44 WARN  [jetty-main-thread-1]  org.sonatype.guice.nexus.scanners.NexusTypeVisitor - Found legacy @org.sonatype.plugin.Managed annotation: com.sonatype.index.rdf.RdfIndexFactory  jvm 1    | 2013-10-04 10:26:44 WARN  [jetty-main-thread-1]  org.sonatype.guice.nexus.scanners.NexusTypeVisitor - Found legacy component relying on @org.sonatype.plugin.Managed magic to automatically imply @javax.inject.Named @javax.inject.Singleton: com.sonatype.index.base.internal.DefaultSearcherRegistry  jvm 1    | 2013-10-04 10:26:44 WARN  [jetty-main-thread-1]  org.sonatype.guice.nexus.scanners.NexusTypeVisitor - Found legacy component relying on @org.sonatype.plugin.Managed magic to automatically imply @javax.inject.Named @javax.inject.Singleton: com.sonatype.index.base.internal.DefaultIndexerRegistry  {quote}  \"","issue_type":"Improvement","issue_priority":"Major","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Clean up use of injection in nexus-custom-metadata-plugin\"","issue_description":"\"There is a lot of crude here due to legacy problems with initial plexus -> guice conversion.  These components can\/should be simplified.\"","issue_type":"Improvement","issue_priority":"Major","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Remove Managed and ExtensionPoint\"","issue_description":"","issue_type":"Improvement","issue_priority":"Major","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"System property http.proxyHost incompatible regular expressions, server wide\"","issue_description":"\"We are running nexus and jenkins on the same tomcat server.  When setting the proxy settings in nexus, these settings are saved in proxy system properties. These properties are shared for everything running in tomcat. Nexus requires the non proxy hosts to be configured as regular expressions. This leads to a system property http.nonProxyHosts=.\\*\\.honda-eu\\.com|.\\*\\.eu\\.honda\\.com. This is however not a valid value according to http:\/\/docs.oracle.com\/javase\/7\/docs\/technotes\/guides\/net\/proxies.html (This is a list of patterns separated by '|'. The patterns may start or end with a '*' for wildcards. Any host matching one of these patterns will be reached through a direct connection instead of through a proxy.).   In short: if nexus uses it's own proxy configuration standards, it should not modify, nor use the system properties.\"","issue_type":"Bug","issue_priority":"Minor","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Yum proxy repository metadata is not refetched if request for it comes through a group repo\"","issue_description":"\"Create a group repository that contains a hosted repository which has yum metadata generation enabled, and also a proxy repository of a remote which contains yum metadata.  Create a merge metadata capability for this group.    Requests for metadata to the group for \"\"repodata\/repomd.xml\"\" never trigger a request to the proxy repository's remote for this file, regardless of cache timeout settings.    This means that new artifacts created on the remote will never become available through the group.\"","issue_type":"Bug","issue_priority":"Major","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Old yum metadata is never cleaned up from yum proxy repository.\"","issue_description":"\"Every time an rpm is deployed into a yum repository it causes a brand new set of metadata files to be created (these all have sha1 sums embedded in their file names).    If you have a proxy of a yum repository, this means a new version of the metadata is downloaded every time, and after a while these consume significant space.    Maybe if repmod.xml is downloaded we should remove the rest of the metadata files from the repodata directory?    {quote}  $ ls  003bfcf89201c1a2ed591531c66effc3a4e027fe641d219377684a9aeba20169-filelists.xml.gz  08828c0881e310aa4dbee6214d077e8e15ad90b6254a01e1598cb69d3cb5c14f-other.sqlite.bz2  150aa0c323606b53090b01d44e73e58ea776df12a2523a99d82e027b908921b0-other.xml.gz  186e3ab67ffc76df9b164dc119b7a5f3cf265fde19da2531e79a9baa5068879d-filelists.sqlite.bz2  189a204d4d0469e105bba086656705cac04c3d1f2eae3b02aa5f3dc4b3b64a97-other.xml.gz  27cad8418cc46a381004ca03cc8b6cd8b74d74710b3ea034704824e45ceebd5a-filelists.sqlite.bz2  2d3d0431515ec9755030d618befb82c9fcee5c10dc6478714d639751eebb3a8b-other.sqlite.bz2  3df248f0d472c45e332dc77ce63634f1332f0a8e358c60975e312adafe0a3013-filelists.sqlite.bz2  54c03951fe73f3e64ba4025674fde4ba6165a2ea97b56c4286a084bf9baf6799-filelists.sqlite.bz2  5c7e42b0f096cbfc6fb2dec96db01164ee34fd55d5f198297b96b91aea7d79d3-filelists.xml.gz  62983c24247ac537efb30fb84e49757ee5692cc8371f278056aa5431eab55370-other.sqlite.bz2  69cc43f574f49aced9f79fb3d0935ba73201de4bde6f8eb01bfd15ee38e7a61a-primary.sqlite.bz2  6eab60c43f89ac10ee6dce5ef815428c0987afc77f09ce92176bd37ee40559c3-filelists.sqlite.bz2  70d7278e115f2d601cb50f4c9898d8bc0b16e647357d4db1423b80ce712cadf1-primary.sqlite.bz2  7b42db3950a9cd7a36f0bfeca78d84e7bd07d4f12c6a2cf2215570f9b8ec1f2a-other.sqlite.bz2  82d364e082b4d24d74a95e8fa9428cf29753252a205799b87fad8984930afd1f-primary.xml.gz  87ce8c5229877bb63e873f20cdc69a8b1d2a0678de1252702c4aed52c415fd95-other.xml.gz  8ced9f96f28df680e23529ebea075e267b4ae0756086a27c85407c93fa53bcf4-primary.xml.gz  8e2dc4494bb4ac5d799034b42bc4afab80d66ed5a466a15a7bc5f92c2b7c8a30-other.xml.gz  91610135649ef8059312f7d935860d272c759bd0edce7cd9fed2bff0b097fe29-other.xml.gz  9c20ae2b5b43e13b0d385bdb70dacaf486ca75c507a2e440c175fe635ba49cfa-other.sqlite.bz2  a87708732e325a1eeff6c3bddbba184d251c5baafcf571e687bc2238958ffc6a-primary.sqlite.bz2  a93b808d31f4cfec7b1ae8463270e0cd6fd30206a16545196c08a810d4890c9e-primary.xml.gz  b5e7ffa0b66e1199ec7580e3bd14f92d9a63cf2eedc979d53880d916f919a0c4-filelists.xml.gz  d9abd67d9db26be5e9303a529965b482505feca7e49f739ccb1a1b7794a31669-primary.sqlite.bz2  e3acecf7d4dc3d35b6757593d65d6840d17d2e68bf4aa4e33cf9b235d7ca4d44-primary.xml.gz  e5288252c53835b15d1a3cfb1ee79d9fabc7e41a4039518f2c5461b04928bc6f-primary.xml.gz  ebabb5cdcfd90b56ba9c1250958c01442f266ba5d0d4c287a99509de5f85f7b3-filelists.xml.gz  ebc9769b8b38af08a612a420bf5107b4a50f9d9517c7c218ca6bc7382aa24f7d-primary.sqlite.bz2  eda629e1037985fa72aeb80c0054d5638fcbb0265624091b369a513ffbc00399-filelists.xml.gz    {quote}\"","issue_type":"Bug","issue_priority":"Minor","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Automatic routing interferes with yum repo metadata\"","issue_description":"\"See here, the automatic routing prefix file contains the yum metadata files:    {quote}  ## repository-prefixes\/2.0  #  # Prefix file generated by Sonatype Nexus  # Do not edit, changes will be overwritten!  \/repodata\/ebabb5cdcfd90b56ba9c1250958c01442f266ba5d0d4c287a99509de5f85f7b3-filelists.xml.gz  \/repodata\/e5288252c53835b15d1a3cfb1ee79d9fabc7e41a4039518f2c5461b04928bc6f-primary.xml.gz  \/repodata\/6eab60c43f89ac10ee6dce5ef815428c0987afc77f09ce92176bd37ee40559c3-filelists.sqlite.bz2  \/repodata\/91610135649ef8059312f7d935860d272c759bd0edce7cd9fed2bff0b097fe29-other.xml.gz  \/repodata\/8e2dc4494bb4ac5d799034b42bc4afab80d66ed5a466a15a7bc5f92c2b7c8a30-other.xml.gz  \/archetype-catalog.xml  \/repodata\/62983c24247ac537efb30fb84e49757ee5692cc8371f278056aa5431eab55370-other.sqlite.bz2  \/com\/mycompany  \/repodata\/a93b808d31f4cfec7b1ae8463270e0cd6fd30206a16545196c08a810d4890c9e-primary.xml.gz  \/repodata\/2d3d0431515ec9755030d618befb82c9fcee5c10dc6478714d639751eebb3a8b-other.sqlite.bz2  \/repodata\/eda629e1037985fa72aeb80c0054d5638fcbb0265624091b369a513ffbc00399-filelists.xml.gz  \/repodata\/186e3ab67ffc76df9b164dc119b7a5f3cf265fde19da2531e79a9baa5068879d-filelists.sqlite.bz2  \/repodata\/54c03951fe73f3e64ba4025674fde4ba6165a2ea97b56c4286a084bf9baf6799-filelists.sqlite.bz2  \/repodata\/189a204d4d0469e105bba086656705cac04c3d1f2eae3b02aa5f3dc4b3b64a97-other.xml.gz  \/repodata\/9c20ae2b5b43e13b0d385bdb70dacaf486ca75c507a2e440c175fe635ba49cfa-other.sqlite.bz2  \/repodata\/70d7278e115f2d601cb50f4c9898d8bc0b16e647357d4db1423b80ce712cadf1-primary.sqlite.bz2  \/repodata\/repomd.xml  \/repodata\/69cc43f574f49aced9f79fb3d0935ba73201de4bde6f8eb01bfd15ee38e7a61a-primary.sqlite.bz2  \/repodata\/8ced9f96f28df680e23529ebea075e267b4ae0756086a27c85407c93fa53bcf4-primary.xml.gz  \/repodata\/d9abd67d9db26be5e9303a529965b482505feca7e49f739ccb1a1b7794a31669-primary.sqlite.bz2  \/repodata\/5c7e42b0f096cbfc6fb2dec96db01164ee34fd55d5f198297b96b91aea7d79d3-filelists.xml.gz  {quote}    These file names (the ones with the embedded checksums) change every time an rpm is deployed into a yum enabled repo.  This means that every time an rpm is deployed the files listed in the repomod.xml file will not be available for download until auto-routing is fetched again, regardless of cache timeout settings.\"","issue_type":"Bug","issue_priority":"Major","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Repository is auto-blocked if \"\"allow file browsing\"\" is disabled on remote\"","issue_description":"\"If you disable \"\"allow file browsing\"\" on a hosted repository, and then create a proxy repository to it in another Nexus instance the repository will be auto-blocked.    We should be able to handle this situation, there are well known paths that can be used for testing such as \"\"\/.meta\/repository-metadata.xml\"\".    Rich  \"","issue_type":"Bug","issue_priority":"Major","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Nexus OBR shadow makes Nexus deadlock prone, while reading\/writing obr.xml\"","issue_description":"\"And this brings down whole instance, as obr.xml processing happens as event handler execute in sync with the operation making event handler kick in (item deploy, delete or so). This leaves guava EventBus SynchronizedEventHandler locked, thunk inhibiting (and blocking) all subsequent thread trying to fire a new event, basically sends Nx down.    Related changes:    Change that went in in Nexus 2.0  https:\/\/github.com\/sonatype\/nexus\/commit\/9ec293d3728610a1fce4283b4f84931ac8f0b9bf    Issue  https:\/\/issues.sonatype.org\/browse\/NEXUS-4682    Notice how event inspector is _async_, exactly to avoid deadlock. Intent was to \"\"decouple\"\" shadows, as they were directly impacting Nx performance.    Then another change went in for Nexus 2.0  https:\/\/github.com\/sonatype\/nexus\/commit\/576e65750442900cbe15553e34021848624da258    Issue  https:\/\/issues.sonatype.org\/browse\/NEXUS-4715    Notice how we made event inspector _sync_ here, this is literally where we introduced the deadlock possibility (as OBR shadow event handling happens in the deployer thread). But, the comment states \"\"this should be async\"\" and we should resolve this.    And later, in 2.6.0, to reduce load on repository registry, a refactoring happened, but processing still kept async  https:\/\/github.com\/sonatype\/nexus-oss\/commit\/c802ac0693ed043b2c85c51c26210a6eaf906875    Related pull  https:\/\/github.com\/sonatype\/nexus\/pull\/156      As we already have issues recorded such as NEXUS-5652, NEXUS-5647 and NEXUS-5642, and many of the imply \"\"rewrite\"\" (anyway needed to support latest OSGI spec), it might be viable to simply drop OBR shadow repositories and implement them in similar way like we had done with P2 or YUM.\"","issue_type":"Bug","issue_priority":"Major","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Cannot browse YUM repodata directory\"","issue_description":"\"When I try to browse http:\/\/nexus:8081\/nexus\/content\/repositories\/releases\/repodata\/ instead of showing listing of this directory web browsers try to download one of the files inside that directory.    This doesn't happen with aliases, I can browse for example http:\/\/nexus:8081\/nexus\/service\/local\/yum\/repos\/releases\/development\/repodata\/    With DEBUG logging I can see this in the logs:  jvm 1 | 2013-07-12 19:44:15 DEBUG [qtp2060873420-40] anonymous org.sonatype.nexus.yum.internal.YumRegistryImpl - Request changed from '\/repodata\/' to '\/repodata\/5c648f3f17376f6d3a096b8e0da53842b1320c34d25b80d9242e9492eaf14d02-filelists.sqlite.bz2'    I think this is caused by matchRequestPath method in SteadyLinksRequestStrategy class.  \"","issue_type":"Bug","issue_priority":"Minor","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Documentation for \"\"Running Bamboo service on Windows as the local user\"\" is missing\"","issue_description":"\"There are no documents\/guidelines for setting up Bamboo to run as a service after Bamboo 50. The page is deleted. Please either add information to: https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Running+Bamboo+as+a+Service+on+Windows or create a new document.\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Include run as Service tutorial in Bamboo Linux Installation Documentation\"","issue_description":"\"Some of the administrator would like to have Documentation in Bamboo in regards of automatic startup or running Bamboo as service is Linux. They need to follow [JIRA documentation|https:\/\/confluence.atlassian.com\/display\/JIRA\/Starting+JIRA+Automatically+on+Linux], and it might not come in handy if they are not aware that JIRA and Bamboo is currently using same application server which is Tomcat.     \\\\  It would be better if it can be included inside [Bamboo installation guide for Linux|https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Bamboo+installation+guide+for+Linux]\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Upgrade Bamboo Dashboard documentation\"","issue_description":"\"The Icons on:   https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Using+the+Bamboo+dashboard?focusedCommentId=407208050#comment-407208050    are out of date.\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Write documentation for configuring the branch checking interval\"","issue_description":"\"Linked to BDEV-3404\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Include documentation for configuring the gravatar server\"","issue_description":"\"As a result of BDEV-3245\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add documentation for the VCS Branching and VCS Tagging tasks\"","issue_description":"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Wrapper references\"","issue_description":"\"This page contains recommendations that include \"\"wrapper\"\" - [https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Configuring+system+properties], but there is no wrapper in Bamboo server starting from version 5.1 as we switched from Jetty to Tomcat. Please update the page accordingly.\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Leave a note on the downloads page and the installation documents about the missing WAR package\"","issue_description":"\"Since Bamboo 5.1 is using Tomcat, we decided to not ship the WAR version to simplify the installation process. But since the package exists for all previous versions it's causing confusion. Please add a note to the download page: https:\/\/www.atlassian.com\/software\/bamboo\/download and\/or the installation\/upgrade documents that explains this: https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Bamboo+installation+guide  https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Bamboo+generic+upgrade+guide\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Missing Documentation for the \"\"Audit Log\"\" feature\"","issue_description":"\"We were unable to find a document that explains the Audit Log feature in Bamboo.(Adminstration->System->Audit Log). When it gets enabled an Audit Log tab gets added in the configuration page for each plan. Certain type of information gets recorded in the \"\"Global Configuration Change History\"\" page and different types of information at the \"\"Plan Level\"\". It will be great to have a detailed, clear document for this feature.\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update screen shots for Setup Wizard docs\"","issue_description":"\"They are still showing the old blue forms.    https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Running+the+Setup+Wizard\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Document applinks and entity links in Bamboo more fully\"","issue_description":"\"There is existing docco, but the story is disjointed.  Should explain app links, entities (e.g. projects).    See:  https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Integrating+Bamboo+with+Confluence  https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Integrating+Bamboo+with+JIRA  https:\/\/confluence.atlassian.com\/display\/BAMBOO\/Creating+JIRA+issues+from+a+build\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Improve the docs for running Bamboo over https\"","issue_description":"\"See   https:\/\/support.atlassian.com\/browse\/BSP-5864?focusedCommentId=2177535&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-2177535\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add warnings in UI about install\/deploy phases\"","issue_description":"\"    what to do in case when \"\"install\"\" or \"\"deploy\"\" phase is declared? deploying instrumented JARs shall be avoided in general          write help for \"\"automatically integrate clover\"\" radio button?          write warning in UI if such goals are found?          write warning in a build log if such goals are found?  \"","issue_type":"Sub-task","issue_priority":"Low","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Insert Clover goals between original ones\"","issue_description":"\"    add \"\"clover2:setup\"\" after the \"\"clean\"\" (if present) and before other goals          but \"\"jaxb2:generate clover2:setup\"\"          but \"\"wsdl2java clover2:setup\"\"  \"","issue_type":"Sub-task","issue_priority":"Low","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"BAM-13208 Automatic integration in multi-module builds\"","issue_description":"\"Follow the approach #1\"","issue_type":"Suggestion","issue_priority":"","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Rename com.cenqua to com.atlassian\"","issue_description":"\"Rename all references to \"\"Cenqua\"\" to \"\"Atlassian\"\". It affects:   * java packages - com.cenqua.* and com_cenqua_clover (\/)   * Clover-for-Eclipse plugins and features (\/)   * com.cenqua.clover:clover artifact (clover core) (\/)    Optional:    Rename also Clover-for-Maven plugin from 'maven-clover2-plugin' to 'clover-maven-plugin'.     (x) The maven plugin will not be renamed. Reason? Typing 'mvn clover:setup' in a project where clover-maven-plugin is not explicitly defined in pom.xml causes that Maven resolves the 'clover:setup' as 'com.atlassian.maven.plugins:maven-clover-plugin:maven-plugin:3.7' which is wrong. So it will be very confusing for customers.    \"","issue_type":"Suggestion","issue_priority":"","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Several tests fail with Cannot cast object ... to class 'com_cenqua_clover.CoverageRecorder'\"","issue_description":"\"See here for a travis build exhibiting this problem:    https:\/\/travis-ci.org\/thehyve\/transmart-core-db\/jobs\/21220075    19 tests there fail because of this problem. The culprit seems to be this line: https:\/\/github.com\/thehyve\/transmart-core-db\/blob\/5e8f0400b9ddce75e1d3d2adccf747e42bff3ee6\/src\/groovy\/org\/transmartproject\/db\/dataquery\/clinical\/TerminalConceptVariablesTabularResult.groovy#L39\"","issue_type":"Bug","issue_priority":"Medium","story_points":3.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"change small class histogram into container\"","issue_description":"\"similarly as for coverage statistics    put into a box:  https:\/\/developer.atlassian.com\/design\/1.2\/containers.html    this:  !small_histogram.png!\"","issue_type":"Suggestion","issue_priority":"","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"change project\/package statistics to the boxed component\"","issue_description":"\"Change this:    !package_summary.png!    Move it under a horizontal navigation (on overview tab for instance). Keep it as a boxed component (div grid). \"","issue_type":"Suggestion","issue_priority":"","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"as a developer I'd like to generate reports with Java8 sources\"","issue_description":"\"New language features might require enhancements of existing reports.     Evaluate how to represent lambdas - for instance:   - as a separate entity in the report, like a class or a method   - as an integral part of enclosing scope - similarly as the inline class is being handled by Clover    Scope:   * HTML, XML, JSON, PDF reports  \"","issue_type":"Suggestion","issue_priority":"","story_points":13.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"as a developer I'd like to instrument Java8 new language features\"","issue_description":"\"This is an enhancement of CLOV-1139. Clover should be able not only to parse Java8 syntax without error, but also be able to add it's own code statements in order to measure code coverage for:  * lambda expressions  * ???  Scope:  * instrumentation (java.g file)  Out of scope:  * database format  * reporting\"","issue_type":"Suggestion","issue_priority":"","story_points":20.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Remove unused remote_address_binary and remote_address_mask columns\"","issue_description":"\"CWD-2420 means that {{remote_address_binary}} and {{remote_address_mask}} are redundant in the {{cwd_application_address}} table. They should be removed from new databases, and possibly dropped from old ones.\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add an extra information in the Issue Security documentation\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-45301].   {panel}  h3. Problem Definition  Today in the https:\/\/confluence.atlassian.com\/display\/JIRA\/Configuring+Issue-level+Security documentation, there is nothing saying the project will assume its permissions in case you don't set an Issue Security to it. Also, the information appearing there can lead the user to think that all issues are available for anyone:    !issue_security.png|thumbnail!    h3. Suggested Solution  Add an extra information in our documentation mentioning the Project Permission will be considered in case there is no Issue Security set to it.  This information in the Issue Security screen would be great as well.\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Time Tracking Reports Documentation Outdated\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-45171].   {panel}  Currently the time tracking reports page states:  {quote}To generate a Time Tracking Report:    Navigate to the desired project.  Choose Summary (tab) > Reports section > Time Tracking Report. {quote}     This is no longer correct in JIRA 6.4. To generate a Time Tracking Report, you must:  Navigate to the desired project.  Choose Reports (bar chart tab) > Forecast &management section > Time Tracking Report    The images on this page are also from an older version of JIRA and should be updated.\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update JIRA REST documentation about removing a user from a project role\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-43999].   {panel}  Actually on the JIRA REST API documentation (https:\/\/docs.atlassian.com\/jira\/REST\/latest\/#api\/2\/project\/{projectIdOrKey}\/role-deleteActor) there's no example to remove a user\/group from a project role:  {quote}  DELETE  Remove actors from a project role.    _available response representations:_    204 [expand]  Returned if the actor was successfully removed from the project role.    404 [expand]  Returned if the project or role is not found, the calling user does not have permission to view it, or does not have permission to modify the actors in the project role.  {quote}    It may lead a customer (and I admit, myself) to use the example code from the POST\/PUT methods on the DELETE method, and it won't work. The DELETE method must be invoked with the following URL:  - http:\/\/JIRA_URL:8080\/rest\/api\/2\/project\/{PROJECTKEY\\}\/role\/{ROLEID\\}?user={username\\}  or  - http:\/\/JIRA_URL:8080\/rest\/api\/2\/project\/{PROJECTKEY\\}\/role\/{ROLEID\\}?group={groupname\\}\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update Creating Issue and Comments documentation\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-43877].   {panel}  On [Creating Issues and Comments from Email|https:\/\/confluence.atlassian.com\/display\/JIRA\/Creating+Issues+and+Comments+from+Email#CreatingIssuesandCommentsfromEmail-Configuringissueorcommentcreationfromemail], we state that the *Catch Email Address* can be used to have multiple handlers reaching the same mailbox:    {quote}  [...]This is useful if you have multiple aliases for the same mail account (e.g. foo-support@example-co.com and bar-support@example-co.com aliases for support@example-co.com) for multiple mail services (e.g. each one to create issues in separate JIRA projects).  {quote}    However, it seems that this is no longer true (see [JRA-42044|https:\/\/jira.atlassian.com\/browse\/JRA-42044]) and we filed a suggestion under [JRA-43829|https:\/\/jira.atlassian.com\/browse\/JRA-43829] to change the functionality in JIRA.    But while this is not implemented, we need to modify the documentation as there are many customers using this configuration scheme that can loose data due to the change in the mail handler behaviour.\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Application Documentation Error\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This bug report is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding bug report|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-43774].   {panel}  Hello,    At https:\/\/confluence.atlassian.com\/display\/JIRA\/Linking+to+Another+Application  an icon is missing in paragraph #2.\"","issue_type":"Bug","issue_priority":"Low","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update Pie Chart Documentation for JIRA Server + Cloud\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-43734].   {panel}  https:\/\/confluence.atlassian.com\/display\/JIRA\/Adding+the+Pie+Chart+Gadget  https:\/\/confluence.atlassian.com\/display\/JIRACLOUD\/Adding+the+Pie+Chart+Gadget  * This gadget now has a new look as well as other changes.    A related page in the AOD space has been changed already. See https:\/\/confluence.atlassian.com\/display\/AOD\/Adding+the+Pie+Chart+Gadget  * This version is restricted and is not publicly accessible.     \"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Upgrading JIRA Manually Instructions update suggestions\"","issue_description":"\"I was performing a manual upgrade following instructions from the following page:  [https:\/\/confluence.atlassian.com\/display\/JIRA064\/Upgrading+JIRA+Manually#UpgradingJIRAManually-3.SettingupyournewJIRAinstallation ]    There were a few sections which were a little unclear and perhaps could be reworded or updated.    Section 2.3 -  {quote}3. Navigate to the directory specified in the configuration file and create a backup of it in another directory.  4. (error) Delete the file <jira-home>\/dbconfig.xml as soon as the backup is complete.{quote}    It's unclear whether the file has to be deleted from the back up or the original folder.  This makes a difference when performing Step 3.2.  It should be clearer that it the file should be removed from the original folder.    {quote}3.2 Point your new JIRA to (a copy of) your existing JIRA Home directory  If your new JIRA 6.4 installation is on a new server, copy the backup of your existing JIRA Home Directory from the old server to the new server before proceeding.{quote}    The user needs to be instructed to keep a backup of the folder and if they are moving to a new server to use a copy of the original folder (which should be minus the dbconfig file).    {quote}3.6 Import your old JIRA data into your new JIRA  After you have started your new JIRA installation, import the data from your old instance into the new instance. You will need the backup file of data from your old JIRA that you created earlier in these instructions (above).    To import your old JIRA data into your new JIRA:    Log in as a user with the 'JIRA System Administrators' global permission.{quote}  After JIRA has been restarted, the user will be directed to the startup wizard - which asks users to set up a new jira instance, or back up from XML.  In order to follow the rest of the instructions the user needs to set up a clean version of JIRA.  This should be mentioned after {quote}After you have started your new JIRA installation,{quote}, to save confusion, or the customer importing the XML twice.\"","issue_type":"Support Request","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Releases notes for 6.1.2 should warn of ORA-01408 error on first start\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-43289].   {panel}  Using Oracle 11g, when doing an in-place upgrade from 6.1.5 to 6.2.1, on the first start of the 6.2.1 application the automatic database modifications causes ORA-1408 :    {code:java}  localhost-startStop-1 ERROR      [core.entity.jdbc.DatabaseUtil] SQL Exception while executing the following:  CREATE INDEX idx_audit_item_log_id2 ON audit_item (LOG_ID)  Error was: java.sql.SQLException: ORA-01408: such column list already indexed  {code}    Then, later, the upgradetask_6256 delete the offending \"\"idx_audit_item_log_id\"\" index.    The idx_audit_item_log_id2 will be created during the next restart of Jira.    It would have been helpful to find this information in the \"\"upgrade notes\"\" for the 6.2.1 version, as seeing an Oracle error in the upgrade logs is very worrying :-)  \"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update JIRA REST documentation about setting users\/groups to a project role\"","issue_description":"\"{panel:bgColor=#e7f4fa}    *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-43054].    {panel}    Actually on the JIRA REST API documentation (_Adds an actor (user or group) to a project role._   - https:\/\/docs.atlassian.com\/jira\/REST\/latest\/) there's wrong example how to set  users\/groups to a project role:  {code}  PUT     Updates a project role to contain the sent actors.    acceptable request representations:        application\/json [collapse]        Example        { \"\"user\"\" : [\"\"admin\"\"] } or      { \"\"group\"\" : [\"\"jira-developers\"\"] }    available response representations:        200 - application\/json [collapse]        Example        {          \"\"self\"\": \"\"http:\/\/www.example.com\/jira\/rest\/api\/2\/project\/MKY\/role\/10360\"\",          \"\"name\"\": \"\"Developers\"\",          \"\"id\"\": 10360,          \"\"description\"\": \"\"A project role that represents developers in a project\"\",          \"\"actors\"\": [              {                  \"\"id\"\": 10240,                  \"\"displayName\"\": \"\"jira-developers\"\",                  \"\"type\"\": \"\"atlassian-group-role-actor\"\",                  \"\"name\"\": \"\"jira-developers\"\"              }          ]      }        Returned if the project and role exists and the user has permission to view it. Contains the role name, description, and project actors.      404 [collapse]        Returned if the actor could not be added to the project role  {code}    Right way is:  {code}  URL: \/rest\/api\/2\/project\/{projectIdOrKey}\/role\/{id}  method: PUT  request body:      {        \"\"id\"\": \"\"10001\"\",  \/\/ actual role id used also in URL        \"\"categorisedActors\"\": {          \"\"atlassian-user-role-actor\"\": [            \"\"user1\"\",            \"\"user2\"\"          ],          \"\"atlassian-group-role-actor\"\": [            \"\"group1\"\",            \"\"group2\"\"          ]        }      }  {code}\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update Configuring JIRA Options documentation regarding Gravatars\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-42853].   {panel}  h3. Problem Definition  Prior to JIRA 6.3, enabling users to use Gravatars in their user profile instead of JIRA-specific avatars caused users not to be able to use JIRA-specific avatars (and vice versa). However, from 6.3 onwards as part of JRA-33596 it is now able to use both together.    However, according to our [Configuring JIRA Options|https:\/\/confluence.atlassian.com\/display\/JIRA\/Configuring+JIRA+Options] documentation:  bq. Enables users to use Gravatars in their user profile instead of JIRA-specific avatars. Users will not be able to use JIRA-specific avatars if Gravatars are enabled, and vice versa.     h3. Suggested Solution  Edit the above to match what is displayed in JIRA - something like:  bq. Allows loading external avatars from a Gravatar server\"","issue_type":"Suggestion","issue_priority":"","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Link broken on Google Apps Integration FAQ\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-40977].   {panel}  At the [Atlassian FAQ for Google Apps|https:\/\/confluence.atlassian.com\/display\/AOD\/Google+Apps+Integration+FAQ ] there's a Link that should lead us to a page describing how to evaluate or buy Ondemmand with Google Apps integration.     The broken link is the following:  [OnDemand listing on Google Apps Marketplace|https:\/\/www.google.com\/enterprise\/marketplace\/viewListing?productListingId=3421+6389312883797462351] \"","issue_type":"Suggestion","issue_priority":"","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update the recommendation for interim upgrade to any version below 4.3 go first to version 4.4.5\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-39485].   {panel}  On the [Skipping Major Versions When Upgrading JIRA|https:\/\/confluence.atlassian.com\/display\/JIRA\/Skipping+Major+Versions+When+Upgrading+JIRA], its second step states that only user with a JIRA version 3.x or below should go to 4.4.5    However, since JIRA 4.3 introduced a big change (Crowd), our team believe it's safer to upgrade first to 4.4.5.  So my request is to update the KB, hence users will have a safer approach to upgrade their instance.     Best regards.  Diego Zarpelon  Atlassian JIRA Support Engineer    \"","issue_type":"Suggestion","issue_priority":"","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Modified the attachments query\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-37070].   {panel}  The attachment query were wrong initially as tested in JIRA 6.2-EAP    Links: https:\/\/confluence.atlassian.com\/display\/JIRA\/Advanced+Searching+-+Fields+Reference#AdvancedSearching-FieldsReference-Attachments    {quote}   * Search for issues which have attachments  {code}  attachments IS EMPTY  {code}  * Search for issues which do not have attachments  {code}  attachments IS NOT EMPTY  {code}  {quote}\"","issue_type":"Suggestion","issue_priority":"","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update instructions for installing on MAC OS X\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Server*. Using *JIRA Cloud*? [See the corresponding suggestion|http:\/\/jira.atlassian.com\/browse\/JRACLOUD-35628].   {panel}  These instructions to install JIRA on Mac OSX need to be updated for Mavericks https:\/\/confluence.atlassian.com\/display\/JIRA\/Installing+JIRA+on+Mac+OS+X#InstallingJIRAonMacOSX-1.DownloadandInstallJIRA    Following instructions on a related Stack Overflow issue (http:\/\/stackoverflow.com\/questions\/19533528\/installing-java-on-os-x-10-9-mavericks):   1. Install the correct version on Java http:\/\/support.apple.com\/kb\/DL1572?viewlocale=en_US  2. Set JAVA_HOME as instructed    Also, the readme file in the download version has instructions for Linux, Solaris & Windows, but not Mac. That's ok, but would be nice to call out that Mac is unsupported, or link to relevant documentation if you want to attempt install on Mac OS X.\"","issue_type":"Suggestion","issue_priority":"","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Column titles in board configuration overflow disasterously\"","issue_description":"\"The problem compounds the more columns are added. Obviously the test data here is unrealistic, but with 6 columns the max chars before overflow is 23\"","issue_type":"Bug","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Bump maven-jira-plugin version to 3.9.1\"","issue_description":"\"AMPS 3.7+ adds the {{Atlassian-Build-Date}} entry to your plugin manifest, and this is used to enforce plugin maintenance expiry. However, GH is currently building with AMPS 3.6. See GHS-4887 for why this is a problem.\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Get GreenHopper 5.1 compatible\"","issue_description":"\"* Usage of {{attr()}} method on things which are not attributes * Lucene date format * JIRA.RestfulTable * jQuery handling of live events when things are \"\"clicked\"\" that aren't in the DOM * anything else that comes up\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\" MVR: Agile issue web panel to display Scrum information of an issue\"","issue_description":"\"This is a complex one, so let's break it down.  * Providing a web panel on the View Issue page to display Scrum information. * Information displayed will be a list of the sprints of which this issue is a part of. * The sprints will be broken down into two sections: Active and Completed * Sprints will provide a link to \"\"go to the sprint\"\". This means going to a Rapid Board for this sprint. * The boards available to go to will be calculated only once the user initiates the action. * Valid boards are calculated based on which boards the user has permission to see, as well as if the issue matches the filter of the board; ** Only scrum boards will be considered. * If there are no valid boards, the user will be shown a message; * If there are multiple valid boards, the user will be prompted to choose one. * If there is only one valid board, or if the user has chosen a board, the user will be redirected instantly. * The user will be redirected to the precise location of the sprint on the chosen board; ** If the sprint is active, the location is Work mode of the board. ** If the sprint is completed, the location is the retrospective report of that sprint on that board.\"","issue_type":"Suggestion","issue_priority":"","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"GreenHopper doesn't respect JIRA's avatar settings\"","issue_description":"\"1. Enable Gravatar support in JIRA 2. Go to the RapidBoard 3. The avatars shown on card are not the Gravatar ones  To fix this use the AvatarService's {{getAvatarURL}} method instead of building your own avatar URL.\"","issue_type":"Bug","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Cycle time getting some very big values in certain conditions\"","issue_description":"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Upgrade task to unify Board\/Sprint data into AO instead of using PropertySets\"","issue_description":"","issue_type":"Suggestion","issue_priority":"","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Sprint marker jumps to bottom of the backlog if you delete an issue in a sprint\"","issue_description":"","issue_type":"Bug","issue_priority":"High","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"As a user I would like to go to GH as my default JIRa home page\"","issue_description":"\"This story involves implementing Trevs MyHome page feature.  Currently this is a per user dark feature.  We could add a -button- affordance to make this available on the board page\"","issue_type":"Suggestion","issue_priority":"","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MV?: Implement the skinny detail view\"","issue_description":"\"Moves as you traverse with n and p   Fixed on RHS with all columns visible) (was Shifts to the right of the currently selected column \"","issue_type":"Suggestion","issue_priority":"","story_points":20.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Cycle time not shown on hover when only one status selected in the control graph\"","issue_description":"","issue_type":"Bug","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"As a control chart user, I want to see actual values of the moving average on hover \"","issue_description":"\"I'd like to see what the various values of the control chart moving average is, especially what the value is on the end date of the graph (I'd like to see that value on a wallboard as well!) - on hover would be the most sensible suggestion on how you'd go about implementing it. \"","issue_type":"Suggestion","issue_priority":"","story_points":0.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MVR: Prompt the user to resolve the parent when the subtasks are all done \"","issue_description":"\"Just for parent swimlane strategy\"","issue_type":"Suggestion","issue_priority":"","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MVR: Implement visualisation of swimlanes as parents \"","issue_description":"\"Needs drop down in the config (refer to design) Parents always present except if themselves and all of their children are filtered  Swimlane configuration retains user defined filters even if you switch to parent strategy  \"","issue_type":"Suggestion","issue_priority":"","story_points":13.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MVR: Issue operation to jump to a rapid board from an Issue\"","issue_description":"\"This is a complex one, so let's break it down.  * Providing an issue operation for an Issue to be able to \"\"go to\"\" a Rapid Board for this issue. * The boards available to go to will be calculated only once the user initiates the action. * Valid boards are calculated based on which boards the user has permission to see, as well as if the issue matches the filter of the board; ** In the case of Kanban boards, the issue must also match the Work mode sub filter. It should *not* match if the issue only appears on Report mode. ** In the case of Scrum boards, the issue may appear in any mode (Plan, Work or Report). * If there are no valid boards, the user will be shown a message; ** _Optionally, the user will be prompted to create a new board based on the project of the issue_. * If there are multiple valid boards, the user will be prompted to choose one. * If there is only one valid board, or if the user has chosen a board, the user will be redirected instantly. * The user will be redirected to the precise location of the issue on the chosen board. ** The issue will be selected if the location is Plan or Work mode. ** In the case of a Scrum board, if the location is the Report mode, and the issue belongs to more than one sprint, the most recent sprint's retrospective report will be chosen.\"","issue_type":"Suggestion","issue_priority":"","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Control graphs should show hours if less than 1d\"","issue_description":"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Column titles in board configuration overflow disasterously\"","issue_description":"\"The problem compounds the more columns are added. Obviously the test data here is unrealistic, but with 6 columns the max chars before overflow is 23\"","issue_type":"Bug","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Bump maven-jira-plugin version to 3.9.1\"","issue_description":"\"AMPS 3.7+ adds the {{Atlassian-Build-Date}} entry to your plugin manifest, and this is used to enforce plugin maintenance expiry. However, GH is currently building with AMPS 3.6. See GHS-4887 for why this is a problem.\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Get GreenHopper 5.1 compatible\"","issue_description":"\"* Usage of {{attr()}} method on things which are not attributes  * Lucene date format  * JIRA.RestfulTable  * jQuery handling of live events when things are \"\"clicked\"\" that aren't in the DOM  * anything else that comes up\"","issue_type":"Suggestion","issue_priority":"","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\" MVR: Agile issue web panel to display Scrum information of an issue\"","issue_description":"\"This is a complex one, so let's break it down.    * Providing a web panel on the View Issue page to display Scrum information.  * Information displayed will be a list of the sprints of which this issue is a part of.  * The sprints will be broken down into two sections: Active and Completed  * Sprints will provide a link to \"\"go to the sprint\"\". This means going to a Rapid Board for this sprint.  * The boards available to go to will be calculated only once the user initiates the action.  * Valid boards are calculated based on which boards the user has permission to see, as well as if the issue matches the filter of the board;  ** Only scrum boards will be considered.  * If there are no valid boards, the user will be shown a message;  * If there are multiple valid boards, the user will be prompted to choose one.  * If there is only one valid board, or if the user has chosen a board, the user will be redirected instantly.  * The user will be redirected to the precise location of the sprint on the chosen board;  ** If the sprint is active, the location is Work mode of the board.  ** If the sprint is completed, the location is the retrospective report of that sprint on that board.\"","issue_type":"Suggestion","issue_priority":"","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Greenhopper license with expired maintenance period does not work in JIRA 5.0.1 and Greenhopper 5.9.1\"","issue_description":"\"We have noticed that Greenhopper with expired maintenance period does not work in anymore in JIRA 5.0.1. Anytime users try to access the agile tab, they are redirected to the plugins page for license update where the license status reads *License is for an older version and maintenance has expired*. The same license however works on JIRA 4.4.4 or 5.0. Please see screenshot for more details.    After the given fix version if this issue is noticed it is likely caused by the a bug in the UPM that has been logged here:  * https:\/\/ecosystem.atlassian.net\/browse\/UPM-2260\"","issue_type":"Bug","issue_priority":"High","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"GreenHopper doesn't respect JIRA's avatar settings\"","issue_description":"\"1. Enable Gravatar support in JIRA  2. Go to the RapidBoard  3. The avatars shown on card are not the Gravatar ones    To fix this use the AvatarService's {{getAvatarURL}} method instead of building your own avatar URL.\"","issue_type":"Bug","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Cycle time getting some very big values in certain conditions\"","issue_description":"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Component dropdown in List View on Planning Board shrinks to 1 character wide in IE8\/Win7 when a lot of fields are added\"","issue_description":"\"Depending on the width of the browser window, the Edit Component dropdown will shrink down to show only the first character of each component under IE8 if there's been a lot of extra fields added to the view.\"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Upgrade task to unify Board\/Sprint data into AO instead of using PropertySets\"","issue_description":"","issue_type":"Suggestion","issue_priority":"","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Sprint marker jumps to bottom of the backlog if you delete an issue in a sprint\"","issue_description":"","issue_type":"Bug","issue_priority":"High","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"As a user I would like to go to GH as my default JIRa home page\"","issue_description":"\"This story involves implementing Trevs MyHome page feature.    Currently this is a per user dark feature.    We could add a -button- affordance to make this available on the board page\"","issue_type":"Suggestion","issue_priority":"","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MV?: Implement the skinny detail view\"","issue_description":"\"Moves as you traverse with n and p   Fixed on RHS with all columns visible) (was Shifts to the right of the currently selected column \"","issue_type":"Suggestion","issue_priority":"","story_points":20.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MV?: When all of the subtasks of a story are complete but the parent is not show an affordance that can be clicked by the user to advance the parent (and try to resolve the issue)\"","issue_description":"\"Clarification: Summary states that the affordance only needs to appear when the parent status is out of sync.    Should the affordance just show the status of the parent  Minimum required\"","issue_type":"Suggestion","issue_priority":"","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Cycle time not shown on hover when only one status selected in the control graph\"","issue_description":"","issue_type":"Bug","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"As a control chart user, I want to see actual values of the moving average on hover \"","issue_description":"\"I'd like to see what the various values of the control chart moving average is, especially what the value is on the end date of the graph (I'd like to see that value on a wallboard as well!) - on hover would be the most sensible suggestion on how you'd go about implementing it. \"","issue_type":"Suggestion","issue_priority":"","story_points":0.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MVR: Prompt the user to resolve the parent when the subtasks are all done \"","issue_description":"\"Just for parent swimlane strategy\"","issue_type":"Suggestion","issue_priority":"","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Performance: Improve load time of the filter list when creating a new board\"","issue_description":"\"When the user is on the getting started page and clicks the \"\"Create a new rapid board\"\" on the kaban tab it takes about 15 seconds to display the popup.    2012-03-07 11:11:29,665 http-80-5 WARN <USER>671x28377x1 czhhp 10.6.240.154 \/rest\/greenhopper\/1.0\/rapidview\/createmodel [<USER>requeststats.filter.ThreadStatsFilter] TIME: 14545 CPU: 8400 8060 HEAP: 457843512    Full details on SAC: https:\/\/support.atlassian.com\/browse\/GHS-4116\"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MVR: Implement visualisation of swimlanes as parents \"","issue_description":"\"Needs drop down in the config (refer to design) Parents always present except if themselves and all of their children are filtered  Swimlane configuration retains user defined filters even if you switch to parent strategy  \"","issue_type":"Suggestion","issue_priority":"","story_points":13.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MVR: Issue operation to jump to a rapid board from an Issue\"","issue_description":"\"This is a complex one, so let's break it down.    * Providing an issue operation for an Issue to be able to \"\"go to\"\" a Rapid Board for this issue.  * The boards available to go to will be calculated only once the user initiates the action.  * Valid boards are calculated based on which boards the user has permission to see, as well as if the issue matches the filter of the board;  ** In the case of Kanban boards, the issue must also match the Work mode sub filter. It should *not* match if the issue only appears on Report mode.  ** In the case of Scrum boards, the issue may appear in any mode (Plan, Work or Report).  * If there are no valid boards, the user will be shown a message;  ** _Optionally, the user will be prompted to create a new board based on the project of the issue_.  * If there are multiple valid boards, the user will be prompted to choose one.  * If there is only one valid board, or if the user has chosen a board, the user will be redirected instantly.  * The user will be redirected to the precise location of the issue on the chosen board.  ** The issue will be selected if the location is Plan or Work mode.  ** In the case of a Scrum board, if the location is the Report mode, and the issue belongs to more than one sprint, the most recent sprint's retrospective report will be chosen.\"","issue_type":"Suggestion","issue_priority":"","story_points":8.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Control graphs should show hours if less than 1d\"","issue_description":"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"chaincode url decode should never return empty urllocation with no error\"","issue_description":"\"Exisiting code will decode  the url \"\"http:\/\/\/\"\" with returning \"\"\"\",nil.    The usage only check `err != nil` and thought it was successful (core\/chaincode\/platform\/golang\/package.go#252).    Actually, in this case the decode should throw error as it's not a valid chaincode location url.  \"","issue_type":"Bug","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"OSX local run chaincode -> Killed: 9\"","issue_description":"\"OSX local run chaincode -> Killed: 9   build peer .\/peer -> Killed: 9   build configtxgen .\/configtxgen  -> Killed: 9   build about fabirc run the program on MacOS -> killed 9  Maybe is my problem\"","issue_type":"Bug","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Implement serialize method of SigningIdentity\"","issue_description":"","issue_type":"Sub-task","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Implement serialize method of Identity\"","issue_description":"","issue_type":"Sub-task","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Support golang 1.8\"","issue_description":"\"golang 1.8 comes out, with better compiling performance.    https:\/\/blog.golang.org\/go1.8    May take golang 1.8 as the default compiler.\"","issue_type":"Story","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"send_transaction\"","issue_description":"","issue_type":"Sub-task","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"create_transaction\"","issue_description":"","issue_type":"Sub-task","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"create_transaction_proposal\"","issue_description":"\"Implement the transaction proposal creation method.\"","issue_type":"Sub-task","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"send_transaction_proposal\"","issue_description":"","issue_type":"Sub-task","issue_priority":"Medium","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"MVP design and implementation - python SDK\"","issue_description":"\"Support chaincode deploy\/invoke with a MVE (1 peer + 1 order + 1 ca).    Working-on volunteers:   * chang   * Latitia   * kai   * <USER>  * tony yang   * dong wang    Welcome for more volunteers.\"","issue_type":"Epic","issue_priority":"High","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Fix unnessary errorf, fatalf statement in configtx package\"","issue_description":"\"Fix unnessary errorf, fatalf statement, use errors.New, t.Error, t.Fatal instead\"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Unstable re-compiling after `make peer` fails \"","issue_description":"\"With latest master branch code (0eadb03506cfde78ea790c6d98d7eb0af98e5842).    1. install those git\/go env.    2. run `make peer`. It may fail on getting gcimporter15 pkg due to network problem sometimes. Or other reason to let the make progress fail once.  ```sh  $ make peer                                                                                                  1 \u21b5  make[1]: Entering directory '\/opt\/gopath\/src\/github.com\/hyperledger\/fabric\/gotools'  Building github.com\/golang\/lint\/golint -> golint  package golang.org\/x\/tools\/go\/gcimporter15: unrecognized import path \"\"golang.org\/x\/tools\/go\/gcimporter15\"\" (https fetch: Get https:\/\/golang.org\/x\/tools\/go\/gcimporter15?go-get=1: dial tcp 74.125.130.141:443: i\/o timeout)  make[1]: *** [gotool.golint] Error 1  Makefile:51: recipe for target 'gotool.golint' failed  make[1]: Leaving directory '\/opt\/gopath\/src\/github.com\/hyperledger\/fabric\/gotools'  Makefile:56: recipe for target '\/opt\/gotools\/obj\/gopath\/bin\/golint' failed  make: *** [\/opt\/gotools\/obj\/gopath\/bin\/golint] Error 2  make: *** [build\/docker\/gotools] Error 2  ```    3. Manually install the gcimporter15 pkg through `go get`, or just ignore.    4. Re-run `make peer` without `make clean`.  ```sh  $ make peer  Installing chaintool  curl -L https:\/\/github.com\/hyperledger\/fabric-chaintool\/releases\/download\/v0.10.0\/chaintool > build\/bin\/chaintool    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                   Dload  Upload   Total   Spent    Left  Speed  100   582    0   582    0     0    442      0 --:--:--  0:00:01 --:--:--   442  100 14.6M  100 14.6M    0     0   166k      0  0:01:30  0:01:30 --:--:--  288k  chmod +x build\/bin\/chaintool  Creating build\/goshim.tar.bz2  mkdir -p build\/image\/ccenv\/payload  cp build\/docker\/gotools\/bin\/protoc-gen-go build\/bin\/chaintool build\/goshim.tar.bz2 build\/image\/ccenv\/payload  cp: cannot stat \u2018build\/docker\/gotools\/bin\/protoc-gen-go\u2019: No such file or directory  make: *** [build\/image\/ccenv\/payload] Error 1  ```  \"","issue_type":"Bug","issue_priority":"Low","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"2.12.0-rc0 logs JMX InstanceAlreadyExistsException at WARNING level\"","issue_description":"\"When running several WAR files under one appserver, and both WAR files create one or several MongoClient instances, with 2.12.0-rc0 we start seeing  {code} WARNING: Unable to register MBean org.mongodb.driver:type=ConnectionPool,clusterId=3,host=localhost,port=27017 javax.management.InstanceAlreadyExistsException: org.mongodb.driver:type=ConnectionPool,clusterId=3,host=localhost,port=27017  at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)  at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)  at com.mongodb.util.management.jmx.JMXMBeanServer.registerMBean(JMXMBeanServer.java:59)  at com.mongodb.JMXConnectionPoolListener.connectionPoolOpened(JMXConnectionPoolListener.java:52)  at com.mongodb.PooledConnectionProvider.<init>(PooledConnectionProvider.java:60)  at com.mongodb.DefaultClusterableServerFactory.create(DefaultClusterableServerFactory.java:50)  at com.mongodb.BaseCluster.createServer(BaseCluster.java:202)  at com.mongodb.SingleServerCluster.<init>(SingleServerCluster.java:45)  at com.mongodb.Clusters.create(Clusters.java:37)  at com.mongodb.DBTCPConnector.start(DBTCPConnector.java:75)  at com.mongodb.Mongo.<init>(Mongo.java:346)  at com.mongodb.Mongo.<init>(Mongo.java:327)  at com.mongodb.MongoClient.<init>(MongoClient.java:268)  at com.mongodb.Mongo$Holder.connect(Mongo.java:782) {code}  This happens for the second WAR file starting (first one works fine, even though it started several instances of MongoClient). If we load only one of the WAR files (no matter which) this does not happen.  This is working fine if using 2.11.4 (and a few earlier versions, for sure) of the driver but broke in 2.12.0-rc0\"","issue_type":"Bug","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Support parallelCollectionScan command\"","issue_description":"","issue_type":"New Feature","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"DBRefBase should implement Serializable\"","issue_description":"\"DBRef and DBRefBase should implement Serializable to match all the other supported BSON types.  Implementation note:  Since DB instances are not serializable, the DB property is marked as transient and will not survive deserialization.  As a result, deserialized DBRef instances can not be fetched using the fetch method.\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Remove autoConnectRetry and maxAutoConnectRetryTime options\"","issue_description":"\"Deprecated in 2.12.0.  Will remove in 3.0.\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Remove support for w = -1\"","issue_description":"\"No other driver has this special write concern.  The constant is being deprecate in 2.12, and we should remove it in 3.0.\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"update description of fsync write concern flag\"","issue_description":"\"Update the description of fsync and j flags in WriteConcern Javadoc so that it accurately describes the current behavior, which is:  - j: If true block until write operations have been committed to the journal. Cannot be used in combination with `fsync`. Prior to MongoDB 2.6 this option was ignored if the server was running without journaling. Starting with MongoDB 2.6 write operations will fail with an exception if this option is used when the server is running without journaling. - fsync: If true and the server is running without journaling, blocks until the server has synced all data files to disk. If the server is running with journaling, this acts the same as the `j` option, blocking until write operations have been committed to the journal. Cannot be used in combination with `j`.\"","issue_type":"Improvement","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Allow acceptable latency difference to be configured via API\"","issue_description":"\"Currently the only way to change the default value is via the com.mongodb.slaveAcceptableLatencyMS system property.  It will be added as MongoClientOptions.acceptableLatencyDifference property.\"","issue_type":"New Feature","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Deprecate com.mongodb.WriteResult#getLastError methods\"","issue_description":"\"h4. In driver 2.x Invoked a *getLastError* command on a same connection on which the previous operation was invoked, but could fail if the connection had been \"\"stolen\"\" by another operation..  h4. In driver 3.x  Writes with WriteConcern stronger than WriteConcern.UNACKNOWLEDGED throws *MongoException*.  That's why to keep compatibility - deprecate WriteResult.getLastError\"","issue_type":"Task","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Allow configuration of heartbeat background threads to be set via API\"","issue_description":"\"The Java driver, as of version 2.9.0, supports automatic failover of mongos instances (JAVA-381). The parameters that determine the wait time when the active mongos becomes unavailable, until choosing a new mongos, are only available as system properties. They cannot be set directly via the driver API. This ticket is to add API support for directly specifying the failover timeout value.  For the record, the relevant system properties, and their default values, are:  {code:java} updaterIntervalMS = Integer.parseInt(System.getProperty(\"\"com.mongodb.updaterIntervalMS\"\", \"\"5000\"\")); updaterIntervalNoMasterMS = Integer.parseInt(System.getProperty(\"\"com.mongodb.updaterIntervalNoMasterMS\"\", \"\"10\"\")); mongoOptionsDefaults.connectTimeout = Integer.parseInt(System.getProperty(\"\"com.mongodb.updaterConnectTimeoutMS\"\", \"\"20000\"\")); mongoOptionsDefaults.socketTimeout = Integer.parseInt(System.getProperty(\"\"com.mongodb.updaterSocketTimeoutMS\"\", \"\"20000\"\")); {code}  The maximum time to failover to a new mongos is:  Sum of: <com.mongodb.updaterIntervalMS> + max(com.mongodb.updaterSocketTimeoutMS, com.mongodb.updaterConnectTimeoutMS, ping time) for each mongos   The new options are:  * MongoClientOptions#getHeartbeatFrequency * MongoClientOptions#getHeartbeatConnectRetryFrequency * MongoClientOptions#getHeartbeatConnectTimeout * MongoClientOptions#getHeartbeatSocketTimeout * MongoClientOptions#getHeartbeatThreadCount\"","issue_type":"New Feature","issue_priority":"Major - P3","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Improve scaffolder logging when yaml file is not valid\"","issue_description":"\"Currently when a yaml file is not valid is hard to figure out the problem, the logs are not clear.     A list of the validation errors should be logged to inform the user of why the yaml is not valid and therefore was not parsed.    \"","issue_type":"Enhancement Request","issue_priority":"Critical","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Scaffolder is not setting complete path for the resource\"","issue_description":"\"Scaffolder is not setting complete path for the resource. If a given resource is in \/resources\/{resourceId} scaffolder is generating the following flow:       <flow name=\"\"post:\/{resourceId}\"\">          <set-payload value=\"\"Hello world!\"\" \/>      <\/flow>  The flow should be as follows:       <flow name=\"\"post:\/resources\/{resourceId}\"\">          <set-payload value=\"\"Hello world!\"\" \/>      <\/flow>\"","issue_type":"Bug","issue_priority":"Critical","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"When generating a mule config file from scratch from yaml, the config-ref in the router is incorrect\"","issue_description":"\"When executing the scaffolder only having a yaml file, the generated config file is incorrect. The config-ref in the router element should not be defined, since the generated configuration does not have a name by default. So instead of     Correct:     {code}  <apikit:config raml=\"\"leagues.yaml\"\" consoleEnabled=\"\"true\"\" consolePath=\"\"console\"\" \/>  <apikit:router \/>  {code}    Invalid:    {code}  <apikit:config raml=\"\"leagues.yaml\"\" consoleEnabled=\"\"true\"\" consolePath=\"\"console\"\" \/>  <apikit:router config-ref=\"\"leagues.yaml\"\" \/>  {code}\"","issue_type":"Bug","issue_priority":"Critical","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Scaffolder cannot find included files in yaml\"","issue_description":"\"When a yaml file has other files included, the mule xml is not generated because it fails to find the included files.     For example:    {code}  %TAG ! tag:raml.org,0.1:  ---  title: Leagues API  version: v1  baseUri: http:\/\/localhost\/api  \/leagues:      name: Leagues      get:          responses:              200:                  body:                      application\/json: !!null                      text\/xml: !!null      post:          body: &league-schema              application\/json:                  schema: !include org\/mule\/module\/apikit\/leagues\/league.json              text\/xml:                  schema: !include org\/mule\/module\/apikit\/leagues\/league.xsd          responses:              201: !!null      \/{leagueId}:          uriParameters:              leagueId:                  type: string                  pattern: '[-a-zA-Z0-9]*'                  minLength: 1                  maxLength: 20          get:              responses:                  200:                      body: *league-schema          put:              body: *league-schema              responses:                  204: !!null          delete:              responses:                  204: !!null          \/teams:              get:                  queryParameters:                      offset:                          name: Offset                          description: result set offset                          type: integer                          required: false                          minimum: 0                          default: 0                      limit:                          name: Limit                          description: result set size                          type: integer                          required: false                          minimum: 1                          maximum: 10                  responses:                      200: !!null    {code} \"","issue_type":"Bug","issue_priority":"Critical","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Generated flows don't have the correct indentation\"","issue_description":"\"The generated flows with the scaffolder don't have the correct indentation.     NOTE: Now it's not happening with the generated flows, but with the rest of the elements. \"","issue_type":"Bug","issue_priority":"Critical","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Scaffolder is duplicating flows\"","issue_description":"\"When executing the scaffolder and a resource specification is present in the yaml file and the xml file, a duplicated flow is generated. \"","issue_type":"Bug","issue_priority":"Critical","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"scaffolder is not generating flows properly\"","issue_description":"\"When executing mvn apikit:create, flows are not being created properly.     Steps to reproduce:   # Create an apikit project using maven archetype  # Go to the folder created and execute mvn apikit:create    Expected result:       <flow name=\"\"get:\/resources\"\"><set-payload value=\"\"Hello world!\"\" \/><\/flow>      <flow name=\"\"post:\/resources\"\"><set-payload value=\"\"Hello world!\"\" \/><\/flow>    Actual result:       <flow name=\"\"get:\/resources\"\">          <set-payload value=\"\"Hello world!\"\" \/>      <\/flow>        <flow name=\"\"get:\/\"\"><set-payload value=\"\"Hello world!\"\" \/><\/flow>      <flow name=\"\"post:\/\"\"><set-payload value=\"\"Hello world!\"\" \/><\/flow>    NOTE: If the mule-config.xml file is updated removing the references to the yaml file created by the archetype, then a new xml file is created with the example yaml's name. \"","issue_type":"Enhancement Request","issue_priority":"Blocker","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"yaml file generated with Maven archetype should have global tag set to tag:raml.org,0.1:\"","issue_description":"\"yaml file generated with Maven archetype has incorrect global tag.     Current global tag: %TAG ! tag:raml.org,1.0:  It should set the following global tag: %TAG ! tag:raml.org,0.1:\"","issue_type":"Bug","issue_priority":"Critical","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update scaffolder mule config with latest changes\"","issue_description":"\"Update scaffolder mule config with latest changes.     Changes to consider for new apikit updates:    1. There will be no more apikit:flow as follows:         <apikit:flow name=\"\"retrieveLeagues\"\" resource=\"\"\/resources\"\" action=\"\"GET\"\">          <set-payload value=\"\"Hello world!\"\" \/>      <\/apikit:flow>    Now, we'll have just flows:         <flow name=\"\"get:\/resources\"\">          <set-payload value=\"\"Hello world!\"\" \/>      <\/flow>    2. Rest-processor:     <apikit:rest-processor config=\"\"raml-example-v1.yml\"\" \/>    will change to     <apikit:router config-ref=\"\"forMoreThanOne\"\"\/>    and outside the flow we need to add the following:   <apikit:config raml=\"\"raml-example-v1.yml\"\" name=\"\"forMoreThanOne\"\" consoleEnabled=\"\"true\"\" consolePath=\"\"newPath\"\"\/>  \"","issue_type":"Bug","issue_priority":"Blocker","story_points":5.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Create script to populate apikit dependences, archetype and maven plugin\"","issue_description":"\"Create script to populate apikit dependences, archetype and maven plugin\"","issue_type":"Bug","issue_priority":"Major","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Raml file created should have .yaml extension\"","issue_description":"\"Raml file created should have .yaml extension as per http:\/\/www.yaml.org\/faq.html \"","issue_type":"Bug","issue_priority":"Major","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update archetype mule config with latest changes\"","issue_description":"\"Update archetype mule config with latest changes.     Changes to consider for new apikit updates:    1. There will be no more apikit:flow as follows:         <apikit:flow name=\"\"retrieveLeagues\"\" resource=\"\"\/resources\"\" action=\"\"GET\"\">          <set-payload value=\"\"Hello world!\"\" \/>      <\/apikit:flow>    Now, we'll have just flows:         <flow name=\"\"get:\/resources\"\">          <set-payload value=\"\"Hello world!\"\" \/>      <\/flow>    2. Rest-processor:     <apikit:rest-processor config=\"\"raml-example-v1.yml\"\" \/>    will change to     <apikit:router config-ref=\"\"forMoreThanOne\"\"\/>    and outside the flow we need to add the following:   <apikit:config raml=\"\"raml-example-v1.yml\"\" name=\"\"forMoreThanOne\"\" consoleEnabled=\"\"true\"\" consolePath=\"\"newPath\"\"\/>  \"","issue_type":"Bug","issue_priority":"Blocker","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Remove EE repositories from pom file generated with Maven archetype\"","issue_description":"\"There are two EE repositories generated in the pom file that need to be removed. \"","issue_type":"Bug","issue_priority":"Critical","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"mvn:apikit create - config file created should be included in mule-config.xml file\"","issue_description":"\"When creating a config based on a yaml file (using mvn:apikit create), the config file created is not included in the already existing mule-config.xml file. If this file is deleted (mule-config.xml) the following error is displayed when installing with maven:     [ERROR] Failed to execute goal org.mule.tools.appkit:mule-appkit-maven-plugin:3.4:mule (default-mule) on project mavenArchetypeTest2: No mule-config.xml or mule-deploy.properties in \/Users\/nataliagarcia\/dev\/mavenArchetypeTest2 -> [Help 1]    \"","issue_type":"Bug","issue_priority":"Major","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Maven archetype does not add port to http endpoint\"","issue_description":"\"When an apikit project is generated using the Maven archetype, the http endpoint address is set as follows:   address=\"\"http:\/\/localhost\/api\"\"    It should be set as follows:   <http:inbound-endpoint port=\"\"8080\"\" host=\"\"localhost\"\" path=\"\"api\"\">\"","issue_type":"Bug","issue_priority":"Major","story_points":1.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Mule 3.2.1 HTTPS outbound endpoint leaking file descriptors\"","issue_description":"\"We have a very simplistic mule application. It takes http requests on port 9000 and forwards them to an https protected endpoint. When injecting data on port 9000 (a few hundred requests) it can be seen in the JMX console (OperatingSystem - Attributes - OpenFileDescriptors) that the amount of open file descriptors keeps increasing. Using the lsof command one can also see that the sockets remain in CLOSE_WAIT. No cleanup seems to occur which causes the mule application to crash once no more file descriptors are available. When forwarding the requests to a pure http endpoint instead of https the issue does not occur.    Please find a sample mule-config.xml attached which helps to reproduce the issue. Before the example can be executed please adapt the paths to your trust store and server key store.\"","issue_type":"Bug","issue_priority":"Critical","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Bookstore example fails to show stats page\"","issue_description":"\"When trying to see stats page of bookstore example instead of stats page this message is shown: *Action was processed successfully. There was no result*    h3. Logs  No error is shown in the logs and no specific bookstore example log is created.  mule_ee.log show the follwing  {code}INFO  2011-12-06 11:23:10,137 [324750153@qtp-1515070110-0] org.mule.transport.servlet.MuleRESTReceiverServlet: Default request timeout for GET methods is: 10000  INFO  2011-12-06 11:23:10,138 [324750153@qtp-1515070110-0] org.mule.transport.servlet.MuleRESTReceiverServlet: feedback is set to: true  INFO  2011-12-06 11:23:10,138 [324750153@qtp-1515070110-0] org.mule.transport.servlet.MuleRESTReceiverServlet: Default content type is: text\/html  INFO  2011-12-06 11:23:10,138 [324750153@qtp-1515070110-0] org.mule.transport.servlet.MuleRESTReceiverServlet: Using payload param name: payload  INFO  2011-12-06 11:23:10,152 [324750153@qtp-1515070110-0] org.mule.lifecycle.AbstractLifecycleManager: Initialising: 'vmQueues.requester.945852687'. Object is: VMMessageRequester  INFO  2011-12-06 11:23:10,153 [324750153@qtp-1515070110-0] org.mule.lifecycle.AbstractLifecycleManager: Starting: 'vmQueues.requester.945852687'. Object is: VMMessageRequester{code}    h3. Steps  * Download Mule 3.2.1  * Start Bookstore example  * Go to stats page (http:\/\/localhost:8083\/bookstore-admin\/services\/stats).  * Instead of stats page this message is shown: *Action was processed successfully. There was no result*  * If stats page is shown try repeating the http request until it shows the message.\"","issue_type":"Bug","issue_priority":"Critical","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Inserting <collection-splitter \/><collection-aggregator \/> into a flow produces unexpected results\"","issue_description":"\"The default behaviour of collection-aggregator seems strange in Mule 3.2.0 and 3.1.2;  intuitively, this is a bug (or class of bugs).  If the following is correct or partially correct then there is probably room for improvement in the docs.    Take the following structure:    <flow ...     ...     <collection-splitter \/>        ...     <collection-aggregator \/>       <transformer ...       <outbound-endpoint ...  <\/flow>    - The above flow will afaics output what is emitted by the collection-aggregator, ignoring any latter transfomer before the outbound-endpoint.  - If the collection-splitter emits sequence ids for the correlation group (e.g. if it's fed a list) the collection-aggregator doesn't seem to make use of it by default. Of course, a collection splitter might also be fed other payload types with no inherent ordering such as sets, where ordered output should not be expected.  There seems to have been related issues in the past as well, e.g. MULE-5844    In the attached config, the output of the outbound-endpoint and the return value of the last transformer are different. And, as per the second point above, the order of elements in the payload is different between the same constant flow input.  If the splitter and aggregator are commeted out, they will show the same message payloads, and the ordering is constant when the input is constant.    \"","issue_type":"Bug","issue_priority":"Critical","story_points":null}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Add an extra information in the Issue Security documentation\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-45301].   {panel}  h3. Problem Definition Today in the https:\/\/confluence.atlassian.com\/display\/JIRA\/Configuring+Issue-level+Security documentation, there is nothing saying the project will assume its permissions in case you don't set an Issue Security to it. Also, the information appearing there can lead the user to think that all issues are available for anyone:  !issue_security.png|thumbnail!  h3. Suggested Solution Add an extra information in our documentation mentioning the Project Permission will be considered in case there is no Issue Security set to it. This information in the Issue Security screen would be great as well.\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Time Tracking Reports Documentation Outdated\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-45171].   {panel}  Currently the time tracking reports page states: {quote}To generate a Time Tracking Report:  Navigate to the desired project. Choose Summary (tab) > Reports section > Time Tracking Report. {quote}   This is no longer correct in JIRA 6.4. To generate a Time Tracking Report, you must: Navigate to the desired project. Choose Reports (bar chart tab) > Forecast &management section > Time Tracking Report  The images on this page are also from an older version of JIRA and should be updated.\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update JIRA REST documentation about removing a user from a project role\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-43999].   {panel}  Actually on the JIRA REST API documentation (https:\/\/docs.atlassian.com\/<USER>REST\/latest\/#api\/2\/project\/{projectIdOrKey}\/role-deleteActor) there's no example to remove a user\/group from a project role: {quote} DELETE Remove actors from a project role.  _available response representations:_  204 [expand] Returned if the actor was successfully removed from the project role.  404 [expand] Returned if the project or role is not found, the calling user does not have permission to view it, or does not have permission to modify the actors in the project role. {quote}  It may lead a customer (and I admit, myself) to use the example code from the POST\/PUT methods on the DELETE method, and it won't work. The DELETE method must be invoked with the following URL: - http:\/\/JIRA_URL:8080\/rest\/api\/2\/project\/{PROJECTKEY\\}\/role\/{ROLEID\\}?user={username\\} or - http:\/\/JIRA_URL:8080\/rest\/api\/2\/project\/{PROJECTKEY\\}\/role\/{ROLEID\\}?group={groupname\\}\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update Creating Issue and Comments documentation\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-43877].   {panel}  On [Creating Issues and Comments from Email|https:\/\/confluence.atlassian.com\/display\/JIRA\/Creating+Issues+and+Comments+from+Email#CreatingIssuesandCommentsfromEmail-Configuringissueorcommentcreationfromemail], we state that the *Catch Email Address* can be used to have multiple handlers reaching the same mailbox:  {quote} [...]This is useful if you have multiple aliases for the same mail account (e.g. <EMAIL> and <EMAIL> aliases for <EMAIL>) for multiple mail services (e.g. each one to create issues in separate JIRA projects). {quote}  However, it seems that this is no longer true (see [JRA-42044|https:\/\/<USER>atlassian.com\/browse\/JRA-42044]) and we filed a suggestion under [JRA-43829|https:\/\/<USER>atlassian.com\/browse\/JRA-43829] to change the functionality in JIRA.  But while this is not implemented, we need to modify the documentation as there are many customers using this configuration scheme that can loose data due to the change in the mail handler behaviour.\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Application Documentation Error\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This bug report is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding bug report|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-43774].   {panel}  Hello,  At https:\/\/confluence.atlassian.com\/display\/JIRA\/Linking+to+Another+Application an icon is missing in paragraph #2.\"","issue_type":"Bug","issue_priority":"Low","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update Pie Chart Documentation for JIRA Server + Cloud\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-43734].   {panel}  https:\/\/confluence.atlassian.com\/display\/JIRA\/Adding+the+Pie+Chart+Gadget https:\/\/confluence.atlassian.com\/display\/JIRACLOUD\/Adding+the+Pie+Chart+Gadget * This gadget now has a new look as well as other changes.  A related page in the AOD space has been changed already. See https:\/\/confluence.atlassian.com\/display\/AOD\/Adding+the+Pie+Chart+Gadget * This version is restricted and is not publicly accessible.   \"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Upgrading JIRA Manually Instructions update suggestions\"","issue_description":"\"I was performing a manual upgrade following instructions from the following page: [https:\/\/confluence.atlassian.com\/display\/JIRA064\/Upgrading+JIRA+Manually#UpgradingJIRAManually-3.SettingupyournewJIRAinstallation ]  There were a few sections which were a little unclear and perhaps could be reworded or updated.  Section 2.3 - {quote}3. Navigate to the directory specified in the configuration file and create a backup of it in another directory. 4. (error) Delete the file <<USER>home>\/dbconfig.xml as soon as the backup is complete.{quote}  It's unclear whether the file has to be deleted from the back up or the original folder.  This makes a difference when performing Step 3.2.  It should be clearer that it the file should be removed from the original folder.  {quote}3.2 Point your new JIRA to (a copy of) your existing JIRA Home directory If your new JIRA 6.4 installation is on a new server, copy the backup of your existing JIRA Home Directory from the old server to the new server before proceeding.{quote}  The user needs to be instructed to keep a backup of the folder and if they are moving to a new server to use a copy of the original folder (which should be minus the dbconfig file).  {quote}3.6 Import your old JIRA data into your new JIRA After you have started your new JIRA installation, import the data from your old instance into the new instance. You will need the backup file of data from your old JIRA that you created earlier in these instructions (above).  To import your old JIRA data into your new JIRA:  Log in as a user with the '<USER>USER> global permission.{quote} After JIRA has been restarted, the user will be directed to the startup wizard - which asks users to set up a new <USER>instance, or back up from XML.  In order to follow the rest of the instructions the user needs to set up a clean version of JIRA.  This should be mentioned after {quote}After you have started your new JIRA installation,{quote}, to save confusion, or the customer importing the XML twice.\"","issue_type":"Support Request","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Releases notes for 6.1.2 should warn of ORA-01408 error on first start\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-43289].   {panel}  Using Oracle 11g, when doing an in-place upgrade from 6.1.5 to 6.2.1, on the first start of the 6.2.1 application the automatic database modifications causes ORA-1408 :  {code:java} <USER>startStop-1 ERROR      [core.entity.jdbc.DatabaseUtil] SQL Exception while executing the following: CREATE INDEX idx_audit_item_log_id2 ON audit_item (LOG_ID) Error was: java.sql.SQLException: ORA-01408: such column list already indexed {code}  Then, later, the upgradetask_6256 delete the offending \"\"idx_audit_item_log_id\"\" index.  The idx_audit_item_log_id2 will be created during the next restart of Jira.  It would have been helpful to find this information in the \"\"upgrade notes\"\" for the 6.2.1 version, as seeing an Oracle error in the upgrade logs is very worrying :-) \"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update JIRA REST documentation about setting users\/groups to a project role\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-43054].   {panel}  Actually on the JIRA REST API documentation (https:\/\/docs.atlassian.com\/<USER>REST\/latest\/#api\/2\/project\/{projectIdOrKey}\/role-setActors) there's wrong example how to set  users\/groups to a project role: {code} PUT   Updates a project role to contain the sent actors.  acceptable request representations:      application\/json [collapse]      Example      { \"\"user\"\" : [\"\"admin\"\"] } or     { \"\"group\"\" : [\"\"<USER>developers\"\"] }  available response representations:      200 - application\/json [collapse]      Example      {         \"\"self\"\": \"\"http:\/\/www.example.com\/<USER>rest\/api\/2\/project\/MKY\/role\/10360\"\",         \"\"name\"\": \"\"Developers\"\",         \"\"id\"\": 10360,         \"\"description\"\": \"\"A project role that represents developers in a project\"\",         \"\"actors\"\": [             {                 \"\"id\"\": 10240,                 \"\"displayName\"\": \"\"<USER>developers\"\",                 \"\"type\"\": \"\"atlassian-group-role-actor\"\",                 \"\"name\"\": \"\"<USER>developers\"\"             }         ]     }      Returned if the project and role exists and the user has permission to view it. Contains the role name, description, and project actors.     404 [collapse]      Returned if the actor could not be added to the project role {code}  Right way is: {code} URL: \/rest\/api\/2\/project\/{projectIdOrKey}\/role\/{id} method: PUT request body:     {       \"\"id\"\": \"\"10001\"\",  \/\/ actual role id used also in URL       \"\"categorisedActors\"\": {         \"\"atlassian-user-role-actor\"\": [           \"\"user1\"\",           \"\"user2\"\"         ],         \"\"atlassian-group-role-actor\"\": [           \"\"group1\"\",           \"\"group2\"\"         ]       }     } {code}\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update Configuring JIRA Options documentation regarding Gravatars\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-42853].   {panel}  h3. Problem Definition Prior to JIRA 6.3, enabling users to use Gravatars in their user profile instead of JIRA-specific avatars caused users not to be able to use JIRA-specific avatars (and vice versa). However, from 6.3 onwards as part of JRA-33596 it is now able to use both together.  However, according to our [Configuring JIRA Options|https:\/\/confluence.atlassian.com\/display\/JIRA\/Configuring+JIRA+Options] documentation: bq. Enables users to use Gravatars in their user profile instead of JIRA-specific avatars. Users will not be able to use JIRA-specific avatars if Gravatars are enabled, and vice versa.   h3. Suggested Solution Edit the above to match what is displayed in <USER> something like: bq. Allows loading external avatars from a Gravatar server\"","issue_type":"Suggestion","issue_priority":"","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Link broken on Google Apps Integration FAQ\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-40977].   {panel}  At the [Atlassian FAQ for Google Apps|https:\/\/confluence.atlassian.com\/display\/AOD\/Google+Apps+Integration+FAQ ] there's a Link that should lead us to a page describing how to evaluate or buy Ondemmand with Google Apps integration.   The broken link is the following: [OnDemand listing on Google Apps Marketplace|https:\/\/www.google.com\/enterprise\/marketplace\/viewListing?productListingId=3421+6389312883797462351] \"","issue_type":"Suggestion","issue_priority":"","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"JQL documentation for searching Labels field\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-40856].   {panel}  https:\/\/confluence.atlassian.com\/display\/JIRA\/Advanced+Searching  The documentation for JQL Searching (link above) doesn't contain any information on how to search for Labels.  Please update regarding how to search for them and which operations can be used.\"","issue_type":"Suggestion","issue_priority":"","story_points":4.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update the recommendation for interim upgrade to any version below 4.3 go first to version 4.4.5\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-39485].   {panel}  On the [Skipping Major Versions When Upgrading JIRA|https:\/\/confluence.atlassian.com\/display\/JIRA\/Skipping+Major+Versions+When+Upgrading+JIRA], its second step states that only user with a JIRA version 3.x or below should go to 4.4.5  However, since JIRA 4.3 introduced a big change (Crowd), our team believe it's safer to upgrade first to 4.4.5. So my request is to update the KB, hence users will have a safer approach to upgrade their instance.   Best regards. <USER>Atlassian JIRA Support Engineer  \"","issue_type":"Suggestion","issue_priority":"","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Modified the attachments query\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-37070].   {panel}  The attachment query were wrong initially as tested in JIRA 6.2-EAP  Links: https:\/\/confluence.atlassian.com\/display\/JIRA\/Advanced+Searching+-+Fields+Reference#AdvancedSearching-FieldsReference-Attachments  {quote}  * Search for issues which have attachments {code} attachments IS EMPTY {code} * Search for issues which do not have attachments {code} attachments IS NOT EMPTY {code} {quote}\"","issue_type":"Suggestion","issue_priority":"","story_points":2.0}
{"sprint_name":"Sprint 4","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Update instructions for installing on MAC OS X\"","issue_description":"\"{panel:bgColor=#e7f4fa}   *NOTE:* This suggestion is for *JIRA Cloud*. Using *JIRA Server*? [See the corresponding suggestion|http:\/\/<USER>atlassian.com\/browse\/JRASERVER-35628].   {panel}  These instructions to install JIRA on Mac OSX need to be updated for Mavericks https:\/\/confluence.atlassian.com\/display\/JIRA\/Installing+JIRA+on+Mac+OS+X#InstallingJIRAonMacOSX-1.DownloadandInstallJIRA  Following instructions on a related Stack Overflow issue (http:\/\/stackoverflow.com\/questions\/19533528\/installing-java-on-os-x-10-9-mavericks):  1. Install the correct version on Java http:\/\/support.apple.com\/kb\/DL1572?viewlocale=en_US 2. Set JAVA_HOME as instructed  Also, the readme file in the download version has instructions for Linux, Solaris & Windows, but not Mac. That's ok, but would be nice to call out that Mac is unsupported, or link to relevant documentation if you want to attempt install on Mac OS X.\"","issue_type":"Suggestion","issue_priority":"","story_points":8.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"All Hands\"","issue_description":"","issue_type":"Story","issue_priority":"","story_points":4.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Create the pub sub program\"","issue_description":"\"A continuation of [] creating the scripts to generate the .robot files that test the single file Java Telemetry programs.\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Moving Work and logistics\"","issue_description":"\"Substantial time was taken out of my week meeting renters to rent my home. Visiting the vet to prepare required paperwork for the move. Visiting dealerships to sell my car. Emailing Erin and the rest of HR to get my ticket planned. Calling the airlines to ensure my dogs can board. Meeting with the movers to move all my household goods.     This task capture that time.\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Learn the Jenkins Robotframework Pipeline\"","issue_description":"\"This task was to learn how robot framework and Jenkins interact with each other. This consists of     - Learning the .robot file syntax and the built in keywords to do various tests.   - The Jenkins manager and how it runs and displays the .robot files online.   - The robotframework_SAL repo and the scripts is contains along with how to run them and what they generate.   - The robotframework_salgenerator repo and the scripts it contains along with how to run them\u00a0\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Write setup.py for ts_sal\"","issue_description":"\"Write a setup.py for ts_sal so that scripts and python code will be placed on the path.\u00a0\"","issue_type":"Story","issue_priority":"","story_points":0.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"PCW 2019\"","issue_description":"\"Attend PCW 2019\"","issue_type":"Story","issue_priority":"","story_points":5.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Image display + File monitor integration\"","issue_description":"\"Image display + File monitor integration    Used threading to get modules running together after not much success with asyncio approach\"","issue_type":"Story","issue_priority":"","story_points":3.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"LSST 2019 meeting\"","issue_description":"\"Attend LSST 2019 project meetings\"","issue_type":"Story","issue_priority":"","story_points":5.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Test and release SAL V4\"","issue_description":"\"Test the new features of SAL V4 and the deployment mechanisms\"","issue_type":"Story","issue_priority":"","story_points":5.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Modify run_atdome.py to use the standard CSC main method\"","issue_description":"\"in ts_ATDome the run_atdome.py script uses custom code instead of the standard main CSC method. That is no longer necessary and it makes a bad example so update it.\"","issue_type":"Story","issue_priority":"","story_points":0.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"PCW2019 - Rob\"","issue_description":"\"Story covering time at the LSST PCW for 2019.\"","issue_type":"Story","issue_priority":"","story_points":5.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Epic to track folks time at the PCE\"","issue_description":"\"This epic is used to track time taken in the PCW (All-hands) meeting\"","issue_type":"Epic","issue_priority":"","story_points":40.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"AT Whitelight returns\"","issue_description":"\"implement control logic for chiller always running when bulb is on\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"ATHexapod update for salobj 4\"","issue_description":"\"Update AT Hexapod for salobj 4 and address any incompatibility that arises\"","issue_type":"Story","issue_priority":"","story_points":3.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Check the M2 MATLAB Tool\"","issue_description":"\"Check the M2 MATLAB tool.\"","issue_type":"Story","issue_priority":"","story_points":1.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Design salobj to Kafka feeder\"","issue_description":"\"Design the salobj to Kafka feeder and negotiate with [~<USER> and [~<USER> about details of the schema. I hope they will be willing to change the Avro schema to match the DDS topics.    Another point to discuss is to consider using the IDL or Python topic data classes, instead of the XML files, to create the Avro schema. However, unless doing such a change requires me to write some code, this is a side issue.    Figure out how to write unit tests for this code.    The product will be a preliminary implementation, not necessarily tested.\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Attend the Project Community Workshop\"","issue_description":"\"This is the event at 8\/12 - 8\/16. The website is at:    [https:\/\/project.lsst.org\/meetings\/lsst2019\/]\"","issue_type":"Story","issue_priority":"","story_points":5.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Review the Documents of Hexapod and Rotator in Phase 2\"","issue_description":"\"This task is to continually review the document of hexapod and rotator. Record the questions and get the knowledge transfer from the previous owner.    The ticket for ssh help is [IHS-2490|https:\/\/jira.lsstcorp.org\/browse\/IHS-2490].\"","issue_type":"Story","issue_priority":"","story_points":3.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Deploy simulators on ncsa-integration-test-stand\"","issue_description":"\"Deploy simulators on provisioned ncsa test-integration-stand and run simulators on them. There is no docker, need to take care of things manually.\"","issue_type":"Story","issue_priority":"","story_points":3.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Write python mitutoyo gauge wrapper\"","issue_description":"\"Write a wrapper for the serial interface to the mitutoyo gauge api.\"","issue_type":"Story","issue_priority":"","story_points":2.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Split develop-env-conda into two images\"","issue_description":"\"Split the develop-env-conda into develop-env-conda which has development tools and base-env-conda which has just the bare deployment tools.\"","issue_type":"Story","issue_priority":"","story_points":1.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Write ts_idl conda recipe and setup.py\"","issue_description":"\"Write a conda recipe for ts_idl. As part of the task write a setup.py for it.\"","issue_type":"Story","issue_priority":"","story_points":1.0}
{"sprint_name":"TSSW Sprint - Aug 5 - Aug 17","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Write Watcher model\"","issue_description":"\"Write the Watcher model: the component that reads alarm rules, listens to the events needed by those rules, and manages the alarms.\"","issue_type":"Story","issue_priority":"","story_points":4.0}
{"sprint_name":"Uranium-235","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Inconsistent status macro colours\"","issue_description":"\"h3. Issue Summary    Status macro uses different background colours in the editor and when viewing the page after. The same in comments.  See attached screenshot.    h3. Steps to Reproduce   # Create page with status macro in it   # Save the page   # Observe status macro look differently than in the editor    h3. Expected Results    Same colours.    h3. Actual Results     !Screen Shot 2019-09-25 at 15.07.31.png!     h3. Workaround     Currently there is no known workaround for this behavior. A workaround will be added here when available\"","issue_type":"Bug","issue_priority":"Low","story_points":2.0}
{"sprint_name":"Uranium-235","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Store temporary files for Atlassian Companion App outside of the Roaming profile on Windows\"","issue_description":"\"h2. *Problem Description*    Editing files via the Atlassian Companion app in Microsoft Windows stores files in two locations:    1. Actual files downloaded and sent to the respective application:    * C:\\Users\\admin\\.atlassian-companion    2. Temporary Cache files (including a CACHE copy of the downloaded file) is stored in:    * C:\\Users\\admin\\AppData\\Roaming\\Atlassian Companion\\Cache    Every time a file is edited, a copy is again put into the above Temporary Cache location inside the *Roaming* folder:    {code}  C:\\Users\\admin\\AppData\\Roaming\\Atlassian Companion\\Cache>dir   Volume in drive C has no label.   Volume Serial Number is DC2E-570C     Directory of C:\\Users\\admin\\AppData\\Roaming\\Atlassian Companion\\Cache    27\/06\/2019  09:52 PM    <DIR>          .  27\/06\/2019  09:52 PM    <DIR>          ..  27\/06\/2019  10:35 PM            45,056 data_0  27\/06\/2019  10:35 PM           270,336 data_1  25\/06\/2019  09:29 AM             8,192 data_2  25\/06\/2019  11:45 AM         4,202,496 data_3  27\/06\/2019  09:22 PM           305,213 f_00000a  27\/06\/2019  09:40 PM         4,305,498 f_00000b  27\/06\/2019  09:41 PM         4,305,498 f_00000c  27\/06\/2019  09:45 PM         4,305,498 f_00000d  27\/06\/2019  09:45 PM         4,305,498 f_00000e  27\/06\/2019  09:46 PM         4,305,498 f_00000f  27\/06\/2019  09:46 PM         4,305,498 f_000010  27\/06\/2019  09:46 PM         4,305,498 f_000011  27\/06\/2019  09:50 PM         4,305,498 f_000012  27\/06\/2019  09:50 PM         4,305,498 f_000013  27\/06\/2019  09:51 PM         4,305,498 f_000014  27\/06\/2019  09:51 PM         4,305,498 f_000015  27\/06\/2019  09:51 PM         4,305,498 f_000016  27\/06\/2019  09:51 PM         4,305,498 f_000017  27\/06\/2019  09:52 PM         4,305,498 f_000018  27\/06\/2019  09:52 PM         4,305,498 f_000019  25\/06\/2019  09:29 AM           262,512 index                21 File(s)     69,676,275 bytes                 2 Dir(s)  14,553,374,720 bytes free  {code}    The *Cache folder* in the Roaming profile can therefore grow excessively large and hit size quotas on the Roaming profile folder.    h2. *Suggestion*    Provide a way to cap the upper limit on the Cache folder.    * [Chromium command-line params|https:\/\/github.com\/electron\/electron\/blob\/master\/docs\/api\/chrome-command-line-switches.md] do support a maximum cache size:    {quote}  *{{--disk-cache-size=size}}*    Forces the maximum disk space to be used by the disk cache, in bytes.  {quote}  * Alternatively, this Cache folder should be written to {{C:\\Users\\admin\\.atlassian-companion}}, outside of the Roaming folder    h2. *Work Around*    Atlassian Companion App uses the Electron framework which  generates\/utilises these two folders:    * Cache  * GPUCache    Edited: It is safe to manually delete the folders - the entire AppData\/Roaming\/Atlassian Companion\/Cache directory should be deleted rather than selectively removing files. If the folder is locked whilst Atlassian Companion App is running please exit the app, delete the folder and restart. This will reduce the disk space used in the Windows Roaming folder.\"","issue_type":"Bug","issue_priority":"Low","story_points":3.0}
{"sprint_name":"Uranium-235","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"Companion App throws System clock error if it starts up before the computer has synced with the NTP server\"","issue_description":"\"h3. Summary  # When connected to the internet, the Companion App checks and compares the computer's time with the NTP time.  # If the time is out of sync, it will throw the error saying \"\"System clock is incorrect\"\"  # Sometimes the Companion App will start before the computer has time to sync with the NTP server, which will them prompt the error message as well.    h3. Steps to replicate  # Set the Companion App to auto start on OS boot.  # Start the computer in another location where the local time is different.  # If the computer boots up, but has not synced with the NTP server to update it's time, the Companion App will throw the \"\"System clock is incorrect\"\" error.    # The issue can also be reproducible by simply changing the local time of the machine, and then setting it to update using the NTP server.  # Start the Atlassian Companion app, and it will throw the error, since it takes some time for the computer to sync with the NTP server time.    h3. Expected behavior  # When starting up, the Companion app should wait awhile for the computer time to sync with the network time, to prevent the error message from appearing.  # Or, there should be an option to disable the checking.    h3. Current behavior   # Companion App throws \"\"System clock is incorrect\"\" error message, as the computer has have the time to sync with the network time.   # In some cases, the app will crash on boot, and but works fine if started manually after.\"","issue_type":"Bug","issue_priority":"Low","story_points":3.0}
{"sprint_name":"v2.0 (should do)","sprint_goal":"Improve system functionality and resolve critical issues.","issue_title":"\"As a chaincode author, I want to set private data in chaincode Init()\"","issue_description":"\"In v1.x, chaincode could not Init() with private data since the collection policy was not yet committed. An error like the following was returned:    \"\"private data APIs are not allowed in chaincode Init\"\"    With new lifecycle, this restriction can be lifted.\"","issue_type":"Story","issue_priority":"Medium","story_points":null}
